{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5287a8d",
   "metadata": {},
   "source": [
    "## Set Global Seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e68a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The global seed 260615\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = 260615\n",
    "set_all_seeds(seed)\n",
    "\n",
    "print(\"The global seed \" + str(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a407e6",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0faaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGUAGE\n",
    "\n",
    "_LANGUAGE_         = 'en'\n",
    "_DATASET_          = '2015'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f81690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CLASSIFICATION\n",
    "\n",
    "_PRETRAINED_LM_    = 'vinai/bertweet-base'\n",
    "_PREPROCESS_TEXT_  = True\n",
    "_TWEET_BATCH_SIZE_ = 5\n",
    "_ADAPTER_CONFIG_   = transformers.ParallelConfig(reduction_factor = 256)\n",
    "_MAX_SEQ_LEN_      = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daabdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "_OUTPUT_DIR_       = 'checkPointsFE'\n",
    "_LOGGING_STEPS_    = 50\n",
    "_NUM_AUTHORS_      = 16\n",
    "_K_FOLD_CV_        = 5\n",
    "_NO_GPUS_          = 1\n",
    "_BATCH_SIZE_       = int(32 / _NO_GPUS_)\n",
    "_EPOCHS_           = 10\n",
    "_LEARNING_RATE_    = 1e-4\n",
    "\n",
    "# PREDICTIONS\n",
    "\n",
    "_PRED_DIR_         = 'FETestGeneralized'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c9214",
   "metadata": {},
   "source": [
    "## Other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b722fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DICTONARIES -----------------------------------------------------------------------\n",
    "\n",
    "# 2015\n",
    "\n",
    "age_dict  = {'18-24': 0, '25-34': 1, '35-49': 2, '50-XX': 3}\n",
    "ageEN_hyp = {0: '18-24', 1: '25-34', 2: '35-49', 3: '50-XX'}\n",
    "ageES_hyp = {0: 'La edad de esta persona es entre 18 y 24 años', \n",
    "             1: 'La edad de esta persona es entre 25 y 34 años', \n",
    "             2: 'La edad de esta persona es entre 35 y 49 años', \n",
    "             3: 'La edad de esta persona es más de 50 años'}\n",
    "\n",
    "# 2017\n",
    "\n",
    "gender_dict    = {'female': 0, 'male':   1}\n",
    "varietyEN_dict = {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}\n",
    "varietyES_dict = {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}  \n",
    "\n",
    "genderEN_hyp  = {0: 'I’m a female', 1: 'I’m a male'}\n",
    "genderES_hyp  = {0: 'Mi nombre es María', 1: 'Mi nombre es José'}\n",
    "\n",
    "# 2019\n",
    "\n",
    "bots_dict  = {'human': 0, 'bot': 1}\n",
    "botsEN_hyp = {0: 'This is a text from a person', 1: 'This is a text from a machine'}\n",
    "botsES_hyp = {0: 'Humano', 1: 'Bot'}\n",
    "\n",
    "# 2020 \n",
    "\n",
    "fakeNews_dict  = {'0': 0, '1': 1}\n",
    "fakeNewsEN_hyp = {0: 'This author is a normal user', 1: 'This author spreads fake news'}\n",
    "fakeNewsES_hyp = {0: 'Este autor es un usuario normal', 1: 'Este autor publica noticias falsas'}\n",
    "\n",
    "# 2021\n",
    "\n",
    "hateSpeech_dict  = {'0': 0, '1': 1}\n",
    "hateSpeechEN_hyp = {0: 'This text does not contain hate speech', 1: 'This text expresses prejudice and hate speech'}\n",
    "hateSpeechES_hyp = {0: 'Este texto es moderado, respetuoso, cortés y civilizado', 1: 'Este texto expresa odio o prejuicios'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc921f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LANGUAGE DICTIONARIES --------------------------------------------------\n",
    "\n",
    "if _LANGUAGE_ == 'en':\n",
    "    age_hyp        = ageEN_hyp\n",
    "    gender_hyp     = genderEN_hyp\n",
    "    variety_dict   = varietyEN_dict\n",
    "    fakeNews_hyp   = fakeNewsEN_hyp\n",
    "    hateSpeech_hyp = hateSpeechEN_hyp\n",
    "    bots_hyp       = botsEN_hyp \n",
    "\n",
    "elif _LANGUAGE_ == 'es':\n",
    "    age_hyp        = ageES_hyp\n",
    "    gender_hyp     = genderES_hyp\n",
    "    variety_dict   = varietyES_dict\n",
    "    fakeNews_hyp   = fakeNewsES_hyp\n",
    "    hateSpeech_hyp = hateSpeechES_hyp\n",
    "    bots_hyp       = botsES_hyp\n",
    "    \n",
    "    \n",
    "# SET LANGUAGE AND DATASET PARAMETERS ----------------------------------------\n",
    "    \n",
    "if   _DATASET_ == '2015':\n",
    "    label_idx  = 2\n",
    "    class_dict = age_dict\n",
    "    label_name = 'age'\n",
    "    label_hyp  = age_hyp\n",
    "    \n",
    "elif _DATASET_ == '2017':\n",
    "    label_idx  = 1\n",
    "    class_dict = gender_dict\n",
    "    label_name = 'gender'\n",
    "    label_hyp  = gender_hyp\n",
    "    \n",
    "elif _DATASET_ == '2019':\n",
    "    label_idx  = 1\n",
    "    class_dict = bots_dict\n",
    "    label_name = 'bots'\n",
    "    label_hyp  = bots_hyp\n",
    "    \n",
    "elif _DATASET_ == '2020':\n",
    "    label_idx  = 1\n",
    "    class_dict = fakeNews_dict\n",
    "    label_name = 'fakeNews'\n",
    "    label_hyp  = fakeNews_hyp\n",
    "    \n",
    "elif _DATASET_ == '2021':\n",
    "    label_idx  = 1\n",
    "    class_dict = hateSpeech_dict\n",
    "    label_name = 'hateSpeech'\n",
    "    label_hyp  = hateSpeech_hyp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c35894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# SET LANGUAGE TOKENIZER\n",
    "\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(_PRETRAINED_LM_)\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef9838",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6438e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET AUTHORS AND LABELS -----------------------------------------------------\n",
    "\n",
    "from tools.DataLoaders import BasePAN\n",
    "\n",
    "baseTrain  = BasePAN(Dir        = 'data/' + _DATASET_,\n",
    "                     split      = 'train',\n",
    "                     language   = _LANGUAGE_,\n",
    "                     label_idx  = label_idx,\n",
    "                     class_dict = class_dict,\n",
    "                     label_name = label_name)\n",
    "\n",
    "baseTest   = BasePAN(Dir        = 'data/' + _DATASET_,\n",
    "                     split      = 'test',\n",
    "                     language   = _LANGUAGE_,\n",
    "                     label_idx  = label_idx,\n",
    "                     class_dict = class_dict,\n",
    "                     label_name = label_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6a42ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e78b60f0-b114-4238-a0b2-dbd52ab12b99',\n",
       " 'eb5cea6d-ecc7-4c1c-b149-dffb4e4d9373',\n",
       " 'c65228f6-59a4-41a9-8d06-788406e5ff7e',\n",
       " 'a1e96b2b-17eb-4450-911b-751a31fadf15',\n",
       " 'ce325322-8731-426d-837a-60ea688fe82d',\n",
       " '541812a8-23ca-40a2-ba5a-2ae0908eada2',\n",
       " '23842323-5699-4551-9ff6-a684dee93a9a',\n",
       " 'f1dcc4ff-0c7c-4fee-9a65-04a13285f327',\n",
       " 'c2253559-eb87-44ac-9a0b-a1d1a0cdfd48',\n",
       " 'ae414abd-cd83-4f0d-bc8c-d0a34a0071a3',\n",
       " '522a6f2f-d308-4b0e-ab31-0ab817e1f5d6',\n",
       " 'f3af2ca2-dbfb-4447-a4f2-dfdfdda58640',\n",
       " '18fc8412-8068-4993-9382-f44d716f092e',\n",
       " 'b7e9f372-21a8-461b-b4f0-950205f84da0',\n",
       " 'dde4cfc8-f4cf-47be-8d9c-21b3b7f63098',\n",
       " '59c19daa-873b-434d-98d9-3b8c0168f946',\n",
       " 'a6323e6d-213e-4105-920c-d375023e5645',\n",
       " '3aae4b14-7dfe-4cb7-b82a-629d01d06da0',\n",
       " '6c51bbf8-f00e-4ddf-8ff5-cf034a6cfbd8',\n",
       " '6a3addbf-f699-482d-ba2f-02248d52db95',\n",
       " '16b423e5-3154-42ef-ad37-3f80858febf5',\n",
       " '1aa8c430-853b-4bbc-b784-df4c88264ccd',\n",
       " 'c4a413a1-1170-4c2c-9f97-3f0bfb474154',\n",
       " 'fde8eb00-0444-4159-9b65-1ead60c2dc88',\n",
       " 'dd453533-94b0-4195-9760-f32d297d8951',\n",
       " '7175696d-d60e-44ed-aebc-1f8582567995',\n",
       " '703806e5-f04b-4e7c-8456-7340784ecf76',\n",
       " '68aa0510-c167-41ae-9c3b-8ac8987db0db',\n",
       " '5a3757da-1eda-4bb5-bf68-816fa10c2231',\n",
       " '34e43885-7fae-4207-80f2-853c0eba099a',\n",
       " '5fea747d-78d3-4baa-915b-1ef6d94c60a7',\n",
       " '61c4aca8-6e7f-4da3-9b30-1f99f7656189',\n",
       " '912ab3dd-fbc1-4bdf-a3e0-07a8ce054ece',\n",
       " '69269faf-28b6-47aa-9fa0-cb4b30c684a6',\n",
       " 'f28ad171-3a6b-4f88-a3c9-feebe7b86747',\n",
       " 'b7bc6377-b203-4a28-a24f-6320b9195041',\n",
       " '81584f97-1167-49e0-8da0-0d4d66f8e238',\n",
       " 'c657394c-bcbd-4b64-8d3a-820011bc8e23',\n",
       " '0aa7bace-924c-40fb-a2e5-3c7012ede244',\n",
       " 'efb846b2-986a-43ae-8b9b-d811f93b52fd',\n",
       " '68b27ee9-bf56-45db-be9a-2e0970d2b6b8',\n",
       " '1661bc1b-1e6a-4c3d-a6b8-ee21f0730288',\n",
       " '25789f19-6181-492d-9f0d-d3b6d07fcc97',\n",
       " '4e165fb2-3dac-413a-a38d-696737dd775b',\n",
       " '3f27d01d-1638-49d3-ade5-84e5372cbc9d',\n",
       " 'e27e308c-e187-4f8d-b4cb-d845f6c9203f',\n",
       " '11c285a3-599c-4fa4-a166-76cccd5b2478',\n",
       " '89fab4e9-24f0-4a97-8e10-ca357cc7865c',\n",
       " '9411c815-9eb6-466e-b3e2-1e739510c94b',\n",
       " '91947b38-5ecd-4b12-957a-26e52198f058',\n",
       " '62bffe2f-1a49-4c79-9f53-ed2e666d41cf',\n",
       " 'ac599f45-3c95-4599-86f1-78a6494011f7',\n",
       " 'aabae9fc-b393-4124-9a1c-32370eb1adfc',\n",
       " '9f4ca5ba-5075-446c-b93a-c0a93ce9bfbc',\n",
       " '5def4eb3-e861-4864-a2f2-669680902007',\n",
       " 'aea6d407-5981-42ce-bccf-1d2bf4094543',\n",
       " 'd7f46166-6a9a-4b2c-8560-04e27d2ea14d',\n",
       " '1d2a2248-1dbf-4e18-933d-42eeedb3ae7e']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET K-FOLD SPLITS -----------------------------------------------------\n",
    "\n",
    "crossVal_splits = baseTrain.cross_val(_K_FOLD_CV_, _NUM_AUTHORS_)\n",
    "\n",
    "crossVal_splits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df486efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 2846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GET TWEETS -----------------------------------------------------\n",
    "\n",
    "baseTrain.get_all_data(_TWEET_BATCH_SIZE_, tokenizer, _MAX_SEQ_LEN_, _PREPROCESS_TEXT_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a92ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7984a07",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c64cf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.DataLoaders import DatasetPAN\n",
    "\n",
    "baseTest.get_all_data(_TWEET_BATCH_SIZE_, tokenizer, _MAX_SEQ_LEN_, _PREPROCESS_TEXT_)\n",
    "\n",
    "Test = DatasetPAN(baseTest, label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36967bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "samples = 2 * _NUM_AUTHORS_ * int(100 / _TWEET_BATCH_SIZE_)\n",
    "_LOGGING_STEPS_ = int(samples / _BATCH_SIZE_)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate               = _LEARNING_RATE_,\n",
    "    num_train_epochs            = _EPOCHS_,\n",
    "    per_device_train_batch_size = _BATCH_SIZE_,\n",
    "    per_device_eval_batch_size  = 200,\n",
    "    logging_steps               = _LOGGING_STEPS_,\n",
    "    output_dir                  = _OUTPUT_DIR_,\n",
    "    save_total_limit            = 10,\n",
    "    overwrite_output_dir        = True,\n",
    "    remove_unused_columns       = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "491d18c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 00:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.485100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.465100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.441500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.361900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.356100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1186.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 1:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8462    0.6600    0.7416        50\n",
      "           1     0.7213    0.8800    0.7928        50\n",
      "\n",
      "    accuracy                         0.7700       100\n",
      "   macro avg     0.7837    0.7700    0.7672       100\n",
      "weighted avg     0.7837    0.7700    0.7672       100\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8250    0.6600    0.7333        50\n",
      "           1     0.7167    0.8600    0.7818        50\n",
      "\n",
      "    accuracy                         0.7600       100\n",
      "   macro avg     0.7708    0.7600    0.7576       100\n",
      "weighted avg     0.7708    0.7600    0.7576       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/5212cb9b5b32726fce956daa9a21ee0a0c2b6e54c54d1af58c678217d85f8143.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/8b04e6b193dae308280a39e7414ec9e55aa85418ff297d0e509550de00b93630.273dce86eeb4b83137e2e5511e720f2949cafbe41d618f259e97749592c88fdb\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 00:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.576300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.533000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.487600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.468700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.446500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.419700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.392700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.409700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.405100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.413300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.419900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.383800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 713.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 2:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7000    0.5600    0.6222        50\n",
      "           1     0.6333    0.7600    0.6909        50\n",
      "\n",
      "    accuracy                         0.6600       100\n",
      "   macro avg     0.6667    0.6600    0.6566       100\n",
      "weighted avg     0.6667    0.6600    0.6566       100\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7073    0.5800    0.6374        50\n",
      "           1     0.6441    0.7600    0.6972        50\n",
      "\n",
      "    accuracy                         0.6700       100\n",
      "   macro avg     0.6757    0.6700    0.6673       100\n",
      "weighted avg     0.6757    0.6700    0.6673       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/5212cb9b5b32726fce956daa9a21ee0a0c2b6e54c54d1af58c678217d85f8143.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/8b04e6b193dae308280a39e7414ec9e55aa85418ff297d0e509550de00b93630.273dce86eeb4b83137e2e5511e720f2949cafbe41d618f259e97749592c88fdb\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 00:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.438900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.369300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.250200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.238400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1230.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 3:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8788    0.5800    0.6988        50\n",
      "           1     0.6866    0.9200    0.7863        50\n",
      "\n",
      "    accuracy                         0.7500       100\n",
      "   macro avg     0.7827    0.7500    0.7426       100\n",
      "weighted avg     0.7827    0.7500    0.7426       100\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9091    0.6000    0.7229        50\n",
      "           1     0.7015    0.9400    0.8034        50\n",
      "\n",
      "    accuracy                         0.7700       100\n",
      "   macro avg     0.8053    0.7700    0.7632       100\n",
      "weighted avg     0.8053    0.7700    0.7632       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/5212cb9b5b32726fce956daa9a21ee0a0c2b6e54c54d1af58c678217d85f8143.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/8b04e6b193dae308280a39e7414ec9e55aa85418ff297d0e509550de00b93630.273dce86eeb4b83137e2e5511e720f2949cafbe41d618f259e97749592c88fdb\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 00:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.497900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.409700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.355800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.336200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.350500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.333000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.333300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1196.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 4:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8919    0.6600    0.7586        50\n",
      "           1     0.7302    0.9200    0.8142        50\n",
      "\n",
      "    accuracy                         0.7900       100\n",
      "   macro avg     0.8110    0.7900    0.7864       100\n",
      "weighted avg     0.8110    0.7900    0.7864       100\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8947    0.6800    0.7727        50\n",
      "           1     0.7419    0.9200    0.8214        50\n",
      "\n",
      "    accuracy                         0.8000       100\n",
      "   macro avg     0.8183    0.8000    0.7971       100\n",
      "weighted avg     0.8183    0.8000    0.7971       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/5212cb9b5b32726fce956daa9a21ee0a0c2b6e54c54d1af58c678217d85f8143.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/8b04e6b193dae308280a39e7414ec9e55aa85418ff297d0e509550de00b93630.273dce86eeb4b83137e2e5511e720f2949cafbe41d618f259e97749592c88fdb\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1280\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 00:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.658100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.490700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.486700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.416600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.408100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.340600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4000\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1242.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 5:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8043    0.7400    0.7708        50\n",
      "           1     0.7593    0.8200    0.7885        50\n",
      "\n",
      "    accuracy                         0.7800       100\n",
      "   macro avg     0.7818    0.7800    0.7796       100\n",
      "weighted avg     0.7818    0.7800    0.7796       100\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8182    0.7200    0.7660        50\n",
      "           1     0.7500    0.8400    0.7925        50\n",
      "\n",
      "    accuracy                         0.7800       100\n",
      "   macro avg     0.7841    0.7800    0.7792       100\n",
      "weighted avg     0.7841    0.7800    0.7792       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from tools.DataLoaders import DatasetCrossVal\n",
    "from transformers import Trainer\n",
    "from tools.Testing import compute_author_predictions\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "task = label_name\n",
    "\n",
    "f1s_soft = []\n",
    "f1s_hard = []\n",
    "\n",
    "for split in range( _K_FOLD_CV_ ):\n",
    "    \n",
    "    # loaders for current split ------------------------------------------\n",
    "    \n",
    "    authors_train, authors_val = crossVal_splits[split]\n",
    "    \n",
    "    Train = DatasetCrossVal(baseTrain, authors_train, task)\n",
    "    Val   = DatasetCrossVal(baseTrain, authors_val  , task)\n",
    "    \n",
    "    \n",
    "    # initialize model ---------------------------------------------------\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(_PRETRAINED_LM_, num_labels = len(class_dict))\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # create trainer and train -------------------------------------------\n",
    "        \n",
    "    trainer = Trainer(\n",
    "        model           = model,\n",
    "        args            = training_args,\n",
    "        train_dataset   = Train,\n",
    "    )\n",
    "    trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "    # get predictions ----------------------------------------------------\n",
    "    \n",
    "    results            = trainer.predict(Test)\n",
    "    author_predictions = compute_author_predictions(baseTest, results.predictions, task, len(class_dict))\n",
    "    \n",
    "    \n",
    "    # report metrics -----------------------------------------------------\n",
    "    \n",
    "    report = {'soft': classification_report(author_predictions['true'], author_predictions['pred_soft'], digits=4), \n",
    "               'hard': classification_report(author_predictions['true'], author_predictions['pred_hard'], digits=4)}\n",
    "\n",
    "    f1s_soft.append( f1_score(author_predictions['true'], author_predictions['pred_soft'], average = 'macro') )\n",
    "    f1s_hard.append( f1_score(author_predictions['true'], author_predictions['pred_hard'], average = 'macro') )\n",
    "\n",
    "    print(\"Results with split \" + str(split + 1) + \":\\n\")\n",
    "    print(\"soft voting:\\n\", report['soft'], '\\n')\n",
    "    print(\"hard voting:\\n\", report['hard'])\n",
    "     \n",
    "    \n",
    "    # save predictions ----------------------------------------------------\n",
    "    \n",
    "    DIR = 'results/' + _DATASET_ + '/' + _LANGUAGE_ + '/' + _PRED_DIR_ + '/' + str(_NUM_AUTHORS_) + '_authors/test_split_' + str(split + 1) + '/'\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    with open(DIR + 'predictions.pickle', 'wb') as f:\n",
    "        pickle.dump(author_predictions, f)\n",
    "\n",
    "    with open(DIR + 'report.txt', 'w') as f:\n",
    "        f.write(\"soft voting:\\n\" + report['soft'] + '\\n\\n')\n",
    "        f.write(\"hard voting:\\n\" + report['hard'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00349e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft results:  [0.767182913250329, 0.6565656565656566, 0.7425599835238389, 0.7863899908452854, 0.7796474358974359]\n",
      "\n",
      "Hard results:  [0.7575757575757576, 0.667305171892328, 0.7631551848419318, 0.797077922077922, 0.779205138498595]\n",
      "\n",
      "\n",
      "Soft statistics: \n",
      "\t[avg, std]: [0.7464691960165093, 0.04737394197468491]\n",
      "\n",
      "Hard statistics: \n",
      "\t[avg, std]: [0.7528638349773068, 0.04493806558996559]\n"
     ]
    }
   ],
   "source": [
    "# report statistics\n",
    "\n",
    "print('Soft results: ', f1s_soft)\n",
    "print('\\nHard results: ', f1s_hard)\n",
    "\n",
    "f1s_soft = np.array(f1s_soft)\n",
    "f1s_hard = np.array(f1s_hard)\n",
    "\n",
    "FewShot_Results = {'soft': [f1s_soft.mean(), f1s_soft.std()], 'hard': [f1s_hard.mean(), f1s_hard.std()]}\n",
    "\n",
    "print('\\n\\nSoft statistics: ')\n",
    "print('\\t[avg, std]:', FewShot_Results['soft'])\n",
    "\n",
    "print('\\nHard statistics: ')\n",
    "print('\\t[avg, std]:', FewShot_Results['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4f9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e598acda",
   "metadata": {},
   "source": [
    "## Training with all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc15e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 2645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.DataLoaders import DatasetPAN\n",
    "\n",
    "baseTest.get_all_data(_TWEET_BATCH_SIZE_, tokenizer, _MAX_SEQ_LEN_, _PREPROCESS_TEXT_)\n",
    "\n",
    "Test  = DatasetPAN(baseTest , label_name)\n",
    "Train = DatasetPAN(baseTrain, label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c7ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "samples = 4 * _NUM_AUTHORS_ * int(100 / _TWEET_BATCH_SIZE_)\n",
    "_LOGGING_STEPS_ = int(samples / _BATCH_SIZE_)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate               = _LEARNING_RATE_,\n",
    "    num_train_epochs            = _EPOCHS_,\n",
    "    per_device_train_batch_size = _BATCH_SIZE_,\n",
    "    per_device_eval_batch_size  = 200,\n",
    "    logging_steps               = _LOGGING_STEPS_,\n",
    "    output_dir                  = _OUTPUT_DIR_,\n",
    "    save_total_limit            = 5,\n",
    "    overwrite_output_dir        = True,\n",
    "    remove_unused_columns       = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "340ae862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2846\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 890\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='890' max='890' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [890/890 01:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.073300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.913900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.890800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.839200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.781900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.830200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.777700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.768600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.744300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.747200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.739400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.711800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsFE/checkpoint-500\n",
      "Configuration saved in checkPointsFE/checkpoint-500/config.json\n",
      "Model weights saved in checkPointsFE/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2645\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 142/142 [00:00<00:00, 1986.07it/s]\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from tools.DataLoaders import DatasetCrossVal\n",
    "from transformers import Trainer\n",
    "from tools.Testing import compute_author_predictions\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "task = label_name\n",
    "\n",
    "# add adapter --------------------------------------------------------\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(_PRETRAINED_LM_, num_labels = len(class_dict))\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# create trainer and train -------------------------------------------\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = Train,\n",
    ")\n",
    "trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# get predictions ----------------------------------------------------\n",
    "\n",
    "results            = trainer.predict(Test)\n",
    "author_predictions = compute_author_predictions(baseTest, results.predictions, task, len(class_dict))\n",
    "\n",
    "\n",
    "# report metrics -----------------------------------------------------\n",
    "\n",
    "report = {'soft': classification_report(author_predictions['true'], author_predictions['pred_soft'], digits=4), \n",
    "           'hard': classification_report(author_predictions['true'], author_predictions['pred_hard'], digits=4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dc3b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8793    0.9107    0.8947        56\n",
      "           1     0.6543    0.9138    0.7626        58\n",
      "           2     1.0000    0.1000    0.1818        20\n",
      "           3     1.0000    0.1250    0.2222         8\n",
      "\n",
      "    accuracy                         0.7535       142\n",
      "   macro avg     0.8834    0.5124    0.5153       142\n",
      "weighted avg     0.8112    0.7535    0.7025       142\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8772    0.8929    0.8850        56\n",
      "           1     0.6386    0.9138    0.7518        58\n",
      "           2     1.0000    0.1000    0.1818        20\n",
      "           3     0.0000    0.0000    0.0000         8\n",
      "\n",
      "    accuracy                         0.7394       142\n",
      "   macro avg     0.6289    0.4767    0.4546       142\n",
      "weighted avg     0.7476    0.7394    0.6817       142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Results:\\n\")\n",
    "print(\"soft voting:\\n\", report['soft'], '\\n')\n",
    "print(\"hard voting:\\n\", report['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8cfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
