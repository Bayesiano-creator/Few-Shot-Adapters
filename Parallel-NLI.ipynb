{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5287a8d",
   "metadata": {},
   "source": [
    "## Set Global Seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e68a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The global seed 260615\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = 260615\n",
    "set_all_seeds(seed)\n",
    "\n",
    "print(\"The global seed \" + str(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a407e6",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0faaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGUAGE\n",
    "\n",
    "_LANGUAGE_         = 'en'\n",
    "_DATASET_          = '2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f81690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CLASSIFICATION\n",
    "\n",
    "_PRETRAINED_LM_    = 'ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli'\n",
    "_PREPROCESS_TEXT_  = True\n",
    "_TWEET_BATCH_SIZE_ = 5\n",
    "_ADAPTER_CONFIG_   = transformers.ParallelConfig(reduction_factor = 256)\n",
    "_MAX_SEQ_LEN_      = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daabdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "_OUTPUT_DIR_       = 'checkPointsParallelNLI'\n",
    "_LOGGING_STEPS_    = 50\n",
    "_NUM_AUTHORS_      = 512\n",
    "_K_FOLD_CV_        = 5\n",
    "_NO_GPUS_          = 1\n",
    "_BATCH_SIZE_       = int(8 / _NO_GPUS_)\n",
    "_EPOCHS_           = 10\n",
    "_LEARNING_RATE_    = 1e-5\n",
    "\n",
    "# PREDICTIONS\n",
    "\n",
    "_PRED_DIR_         = 'pretrainedParallelNLI'\n",
    "_PRETRAINED_ADAPTER_ = '2017_ParallelNLI'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c9214",
   "metadata": {},
   "source": [
    "## Other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b722fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DICTONARIES -----------------------------------------------------------------------\n",
    "\n",
    "# 2015\n",
    "\n",
    "age_dict  = {'18-24': 0, '25-34': 1, '35-49': 2, '50-XX': 3}\n",
    "ageEN_hyp = {0: '18-24', 1: '25-34', 2: '35-49', 3: '50-XX'}\n",
    "ageES_hyp = {0: 'La edad de esta persona es entre 18 y 24 años', \n",
    "             1: 'La edad de esta persona es entre 25 y 34 años', \n",
    "             2: 'La edad de esta persona es entre 35 y 49 años', \n",
    "             3: 'La edad de esta persona es más de 50 años'}\n",
    "\n",
    "# 2017\n",
    "\n",
    "gender_dict    = {'female': 0, 'male':   1}\n",
    "varietyEN_dict = {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}\n",
    "varietyES_dict = {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}  \n",
    "\n",
    "genderEN_hyp  = {0: 'I’m a female', 1: 'I’m a male'}\n",
    "genderES_hyp  = {0: 'Mi nombre es María', 1: 'Mi nombre es José'}\n",
    "\n",
    "# 2019\n",
    "\n",
    "bots_dict  = {'human': 0, 'bot': 1}\n",
    "botsEN_hyp = {0: 'This is a text from a person', 1: 'This is a text from a machine'}\n",
    "botsES_hyp = {0: 'Humano', 1: 'Bot'}\n",
    "\n",
    "# 2020 \n",
    "\n",
    "fakeNews_dict  = {'0': 0, '1': 1}\n",
    "fakeNewsEN_hyp = {0: 'This author is a normal user', 1: 'This author spreads fake news'}\n",
    "fakeNewsES_hyp = {0: 'Este autor es un usuario normal', 1: 'Este autor publica noticias falsas'}\n",
    "\n",
    "# 2021\n",
    "\n",
    "hateSpeech_dict  = {'0': 0, '1': 1}\n",
    "hateSpeechEN_hyp = {0: 'This text does not contain hate speech', 1: 'This text expresses prejudice and hate speech'}\n",
    "hateSpeechES_hyp = {0: 'Este texto es moderado, respetuoso, cortés y civilizado', 1: 'Este texto expresa odio o prejuicios'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc921f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LANGUAGE DICTIONARIES --------------------------------------------------\n",
    "\n",
    "if _LANGUAGE_ == 'en':\n",
    "    age_hyp        = ageEN_hyp\n",
    "    gender_hyp     = genderEN_hyp\n",
    "    variety_dict   = varietyEN_dict\n",
    "    fakeNews_hyp   = fakeNewsEN_hyp\n",
    "    hateSpeech_hyp = hateSpeechEN_hyp\n",
    "    bots_hyp       = botsEN_hyp \n",
    "\n",
    "elif _LANGUAGE_ == 'es':\n",
    "    age_hyp        = ageES_hyp\n",
    "    gender_hyp     = genderES_hyp\n",
    "    variety_dict   = varietyES_dict\n",
    "    fakeNews_hyp   = fakeNewsES_hyp\n",
    "    hateSpeech_hyp = hateSpeechES_hyp\n",
    "    bots_hyp       = botsES_hyp\n",
    "    \n",
    "    \n",
    "# SET LANGUAGE AND DATASET PARAMETERS ----------------------------------------\n",
    "    \n",
    "if   _DATASET_ == '2015':\n",
    "    label_idx  = 2\n",
    "    class_dict = age_dict\n",
    "    label_name = 'age'\n",
    "    label_hyp  = age_hyp\n",
    "    \n",
    "elif _DATASET_ == '2017':\n",
    "    label_idx  = 1\n",
    "    class_dict = gender_dict\n",
    "    label_name = 'gender'\n",
    "    label_hyp  = gender_hyp\n",
    "    \n",
    "elif _DATASET_ == '2019':\n",
    "    label_idx  = 1\n",
    "    class_dict = bots_dict\n",
    "    label_name = 'bots'\n",
    "    label_hyp  = bots_hyp\n",
    "    \n",
    "elif _DATASET_ == '2020':\n",
    "    label_idx  = 1\n",
    "    class_dict = fakeNews_dict\n",
    "    label_name = 'fakeNews'\n",
    "    label_hyp  = fakeNews_hyp\n",
    "    \n",
    "elif _DATASET_ == '2021':\n",
    "    label_idx  = 1\n",
    "    class_dict = hateSpeech_dict\n",
    "    label_name = 'hateSpeech'\n",
    "    label_hyp  = hateSpeech_hyp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c35894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bart to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(_PRETRAINED_LM_)\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "config             = PretrainedConfig.from_pretrained(_PRETRAINED_LM_)\n",
    "nli_label2id       = config.label2id\n",
    "is_encoder_decoder = config.is_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef9838",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6438e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET AUTHORS AND LABELS -----------------------------------------------------\n",
    "\n",
    "from tools.DataLoaders import BasePAN\n",
    "\n",
    "baseTrain  = BasePAN(Dir        = 'data/' + _DATASET_,\n",
    "                     split      = 'train',\n",
    "                     language   = _LANGUAGE_,\n",
    "                     label_idx  = label_idx,\n",
    "                     class_dict = class_dict,\n",
    "                     label_name = label_name)\n",
    "\n",
    "baseTest   = BasePAN(Dir        = 'data/' + _DATASET_,\n",
    "                     split      = 'test',\n",
    "                     language   = _LANGUAGE_,\n",
    "                     label_idx  = label_idx,\n",
    "                     class_dict = class_dict,\n",
    "                     label_name = label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6a42ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ur6ur61c96tkp8r4v5og5wo9r32ctlcc',\n",
       " 'n3l9sc6sinb2up1qxytkppkaq2cu4hd0',\n",
       " '8n0ou578obejna7w0l0ha2ttl7u9jv1z',\n",
       " '2zszbxip2txqztt22cogmmnatf3egx3f',\n",
       " 'ae90k3wgo50si8sd1r5q99uxtnkvpdpi',\n",
       " 'y90uzkbhswi7ii3m3l7pkeha0kcjixxm',\n",
       " 'ei06d61s381rbeorcyd47fhbtih73t6i',\n",
       " 'gpn5y1k1yova9y728z6wp47wxvtscu1i',\n",
       " 'l4e0nfw0yzzaouhu9efltwdk6m95jloj',\n",
       " 'v5lghqolgdvl1b2w9upffzv8g5ge5gbp',\n",
       " '4i1i9kgfahbizuk4rlrrqlh5tppd34zt',\n",
       " 'x9s7evgt3nbpi9qsa4h1y4gn7rdg26op',\n",
       " 'dgm54nxhyr9cextksr1tnflxu416sama',\n",
       " '2mczu9ibksh2qsazz0qkgrbj7bdk87y6',\n",
       " '98as6suith8cskycmudejdvhvmiwi1iy',\n",
       " 'vj3jr31xyw7tsckiiwnqu69yt3smj3wk',\n",
       " 'x5xvdmiagqijl0uoe001wwekdfsw823b',\n",
       " 'nlf7wmx2aczynvwq8s2uhc9vi65itx9n',\n",
       " 'l2a64i3p35nquw35cxneilk4i1an161t',\n",
       " 'eju3ygk3fxhk3snll2phzg4dvpkx9pvj',\n",
       " 'w73115nc8smn0t3xr1ojqp6dqpb1tq35',\n",
       " '18n59cv9zps9sm601zu95z680wma6cvw',\n",
       " 'i8ebqio39p9mb8g7kc2tqv7qdsfurmfs',\n",
       " '6rbqri4fyu77yr9yxtc3r5x7hti49u53',\n",
       " 'pey076bqc5zdegsc9la3vd96vafl4hgz',\n",
       " 'jztc6t415ocxtnioyvgzddv49j3pcus4',\n",
       " '99u7e2kcxz3d8fzaqblow0fgl9u06eeg',\n",
       " 'kw8tsmtmqgfloq4ucb9hbizt7vhwieug',\n",
       " 'o4wkq0eenu9ulwe6st0tqmtyydghw11w',\n",
       " 'oh97ndodfecyygydoh15xtcehksgre9w',\n",
       " 'zpqlckcomr2ers9guqn5zqdzkxcghrfa',\n",
       " '7idtl91310dbx0jzauf5c5dlpu4ico7e',\n",
       " 'q30p8rrnxl5jld9j35f5vx7u6epuf2gr',\n",
       " 'z9sjtv730uwrqxjtr4gc6l9w0j1oczgq',\n",
       " '4whbe0y9fno5nfmxnqzwwydx5oqqy3py',\n",
       " 'bgszxe71rzsv3tmo1qhuriaf8s3a5awc',\n",
       " 'a0b2zb9018211kkel80gr80c3px5mvhc',\n",
       " 'xuf151z5jzhhj2er2adkxed1zcbhjykc',\n",
       " 'heumv2vt7nk9tz7sto58s4q3dxmgxueq',\n",
       " '2m385le0s10tvxfizbktm4sjj7xh506i',\n",
       " 'e3efkh8l3ivtx83ivemenaoicedue5wj',\n",
       " '2fs670xfe9yw8ydstcxicvsqzrdaa87b',\n",
       " 'obszsrq3d4un5rcca947w13fged1birq',\n",
       " 'f4zgi7ym7829iqld6x77q6mh30s0rf86',\n",
       " 'l34d9zzi2gk1784oqmeycg8hffjr5t18',\n",
       " 'j7hxgnl03djtinolkcszy4gv4t1xp0dm',\n",
       " 'd8y3v2kh07gdtlym8k8yjorulc6gua92',\n",
       " 'pun1empp6oyjujfvrtfrq7t21bndapau',\n",
       " '5uoku9g7ox972ybb4axiiwn16717firm',\n",
       " 'de4h65iivu8c2iympl371dgaaj3px9ka',\n",
       " 'hvdrhgkik7svztwi5h151fj3023nuwx2',\n",
       " '0dwovd7nj6yg9m795ng2c629me0ccmrh',\n",
       " 'ae4d8z4xa1zpaex8je25anuz7k3vavzc',\n",
       " 'y2aa8tmzuj9o7bphyo0epsgwmzklp7lz',\n",
       " 'ye2iekzvkven8ztwqlili7eh13jvltoz',\n",
       " 'pgrhppipzisb3xnxhnomjt9udg358l58',\n",
       " 'aidphjbj9c6qakq6j5m1nfbmmiy73f9k',\n",
       " 'gcqzgv3cp8dnm31ys5uy2b8pifq2hhay',\n",
       " '2ieolo0go8fo8wm0str85ls8s7m3tx1k',\n",
       " 'ohcq9l9avafjhna9lxw10b1v3898ew7a',\n",
       " 'hu8w9emfx8y4fiodbe9uvd745cluu8og',\n",
       " '3sdc75b10yx0ixbpda40ckfhpc8d2u8q',\n",
       " 'e0sqnlig8tfs3nggqs4ppm21n42nd1y9',\n",
       " 'nt0kallmiu3m93jvrwgll44wx6ooqnsa',\n",
       " '4so0t8vnmfcyw840z1mswu2td18cfjbr',\n",
       " '8ssqkoq1idfdbhzyw45gmimai30stu34',\n",
       " '1nmrlqpzwa5z7pd36zy28qug14gue00t',\n",
       " '48k5r0j4btqu8rdoq6s16ejkt01onafb',\n",
       " 'inpczyd9ainftnew4ie099z1yds9c8gt',\n",
       " 'iejmhz7ospkdpfy5ma3uc4b9ry4a764b',\n",
       " 'fsrnc43crfcw20q5jv6rl620qluckgjx',\n",
       " 'texx4na58bvzj89l8j19lrsf3e6jxkkw',\n",
       " 'zci96dg47sjje2bnq3xedf3xitiv3q6d',\n",
       " 'sar6hff7d0nogdywvb0vkzivwir7cf3x',\n",
       " '4i44lz82bo72xtgulhmbsqamzt91yn8u',\n",
       " 'ryzyq2dmk3gg9oinvwi2ujutu0jzs4iw',\n",
       " '5we816p2ml0mujwg2ha1vicr0p2qtoo3',\n",
       " 'ghcnqnle4ccmwqdjud69az45sux0uqh2',\n",
       " 'gopyguzecqwdmfvh3ggsui9s5q7nzpnc',\n",
       " '5tm0ls7cvjptgb7ak0367o6xxsakw2hh',\n",
       " 'nrajkdsw3k66fs6xmrcrg5cx59wh6431',\n",
       " 'i8vepz3fjp5yf8z2ns2we6nry2k4jr8t',\n",
       " '5rh7fov0yf6vnayzixjck46q4edm2xma',\n",
       " 'xn1ntk5orulu90oq0vzp1xmhyy9tfno3',\n",
       " 'fbp0q8auxifvaw26muxk08h0e43l5xq7',\n",
       " 'o9pw2tnf0paqtbnatdk4z9hz8nvgbml0',\n",
       " 'zuelpgcp4186rxrhifbslyrdfhhaxxt8',\n",
       " '8y3vw5wsbj8lefvmwar3v297dbkd4hiq',\n",
       " 'd6shpn9ebosxt4n1hhl64benshbezqnx',\n",
       " 'iqx2tqavqtpmh1kdn5wnt2wvlvt4me97',\n",
       " 'q4om4iaf2uvwyvjzifd6kcvuul5xm8a4',\n",
       " 'iehecqzngnxahj29eo7zzqi8t0kuicjj',\n",
       " '5i4wtm84ni0il7sciwmq0uncbtlwvmb0',\n",
       " '816gpz0jt4jnjmb1osav67xw5igbp2af',\n",
       " 'os3r35ujbthdlbyjp815how8w1k3lp48',\n",
       " 'fjo9rdodcq8gu0rp9cluyjmoclotqv00',\n",
       " '4cr6a8eebr9sa7fcv6vgqbmdg4l033v2',\n",
       " 's8mcuywaodqy109hi550lj10q3s30r5h',\n",
       " '526gs9dqx6wissrl3meyisg9i6th6avn',\n",
       " 'lgetudqf25r56kapeymcv95x8q7moka8',\n",
       " '8yj5qg4qz9jynky1imc2aiotp6uwjwxm',\n",
       " 'jmkq5i3hibtrv4v20g9hjnkplucfai6d',\n",
       " '071nxc49ihpd0jlfmvn2lghtayy3b5n9',\n",
       " '1pafet4024z3sdkg590keixo01c2frgr',\n",
       " '14ka43f4ho6puh4iyhrfzsbrdpy5yixi',\n",
       " 'r4o5eh2kmy7ebfnydk3o0kvlykulwgg1',\n",
       " 'kyh917b2w9vecn6ti343cr4tdpux2yqw',\n",
       " 'l7o357ot2banopizg7vid5tbso9h4y3m',\n",
       " 'qp163z3rppzfcf7u6d8qp2oen6p210t8',\n",
       " 'enu4kub4kn25pp392at2kn2pg9dn5tpv',\n",
       " '5hmfbulj8wbmeu51d8k62shniptu9zw3',\n",
       " 'r3fup9twcc1b22vl23ry2npckwl437xo',\n",
       " '6vxnq8zfpipevsg703ahald3s5w9pjcc',\n",
       " 'zwfesexkazacsz78p8g1h6ockrcvoypf',\n",
       " 'blxku3vuct7gkjz53hkz1gxnaojrsmkg',\n",
       " 'syphkjiitpehetpk3avmh2tnfayhlbnb',\n",
       " '36w54uf2991wiao1halknad31ezvotiz',\n",
       " 'ogecvmquvmgwazab0osmth2eaieiejsv',\n",
       " 'sl3rfi2vsl1ambrnean1iwesvbog40x2',\n",
       " 'j6eo0r4csyzmbvtv0ty9cfhaujdknb0j',\n",
       " 'cdoehmckcuu2phkj0g84odpu4kj9f230',\n",
       " '2sqhe8tt1jeoblqs3n6uo0ztvzeika0e',\n",
       " '0s308xu8rhhuqlj0jbh49z0kdx5sr2b9',\n",
       " 'e2lxjfsgoej3zcy11zwjo157p5usis34',\n",
       " '3km44k1va2y4knrz6dlkf2u4kym34y0h',\n",
       " 'ap52gs0xsrckqp5ot84z4qeqx133uu39',\n",
       " 'c2afzs5zoty5d6x06k1m8781j3ie1yah',\n",
       " '2if1gnjcokhd3xf5ohbj0nnunw3puxvf',\n",
       " 'dh252lx7psysxa7uqz2kacizqlfsxqhy',\n",
       " 'xb9ig94k4epico5uwz5bp0ijzeqq8s6h',\n",
       " 'ol7cv5k5hz7rlpow96675tkf4lwjf6ag',\n",
       " 'k8rmuzmt7952ckx7c163ld6w0lob9j3k',\n",
       " 'u77esmcig7ii875cq5cv8pev5qn6xdb7',\n",
       " '36owol1jryds89i6sq94cqwi0d8uthkg',\n",
       " '3rczjyykiaxqpbeabthht21732hxm0d3',\n",
       " 'jnxjj86slokqd3qxz4t0xd1sbh0pnk1g',\n",
       " '06ct0t68y1acizh9eow3g5rhancrppr8',\n",
       " 'lavu4pc9zwshidetx4yof9p7g614gv7g',\n",
       " 'euwc18dw62yiizeyevmdk40enhbjy5vp',\n",
       " 'ljz94z7vs4bm5wawq4141xm6ieonio8o',\n",
       " '1lvdfttfby38rwxqafl7muadiy7mfft5',\n",
       " 'ndqzmihoau9uu9nfm0gkbids9yqu49at',\n",
       " 'q91ww3hmw5lof951ts61p3mu52oxrl3o',\n",
       " 'jc9m8zuhcr9yhtqhvttea9phyyl0n9lx',\n",
       " '3efm6gac2ippn5rd7hq3cqfh6194oqmz',\n",
       " '26ty7w3iixzobweyinhjne1bo7bx3u5d',\n",
       " 'iyamqcg0w2b7n2ukim49aerg3bumyxh4',\n",
       " '9i4x0l6lzayvpooc4ybugncxxd58dvee',\n",
       " '3zdfak10z9plvabj8dwau3zkmbgtcpjo',\n",
       " 'agj1zc5wxirn0885t1cj5gsnhitoacky',\n",
       " 'ns1f9qfr328rgouh298x47g25bet33it',\n",
       " 'zurv8xodgwcle1guhjai6n1i4cw4lc8r',\n",
       " '71jryulbmhbfhue9bfwhk7lfawddealx',\n",
       " 'h56srndge9i25ksdtajjfgsm10e59kgb',\n",
       " 'jcn5hwgaeatzjmjww98ejl6o4omh5n7o',\n",
       " 'cdaj65qctw066ou7x4pn9hc6tjcmeme9',\n",
       " 'xpmlqatcyx250189qrc2vfbx0bg0p8fi',\n",
       " 'f8vlyg0wtbsmu9p9t3egonfs8rqjn6tp',\n",
       " 'cdpexkrbjob7puu8ahw2xj6lt8umj49v',\n",
       " 'pyshaf7jwywzfkafcepizgkbdide8ypf',\n",
       " 'vicffjlhtet4thithalfjj8ngcarykw8',\n",
       " '3x6f8m8j6hh19vqb67bhq9bbs4goyf1j',\n",
       " '7l4pmmmbgv9sacq4p9lv7visshvd4byx',\n",
       " '9bwpkz1tsw7h8w57hs6fjpw41iusdjxn',\n",
       " 'nd9t6ta0k01m71dhg7z1i4zbl07iomw7',\n",
       " 'j7vxxogwiz792uj1uoyhhqf2870ogdgn',\n",
       " 'zw2pjht6tf3ymkfbfbm83zcjxfuumzal',\n",
       " 'rb0jg4y42rgesbbn9yi6ijx0uvf8fijs',\n",
       " 'orv973hw22cy626ws0xtviju1x0tjl5b',\n",
       " 'z6ejdyu1emh9z9mlsnt5km2suhgbonl9',\n",
       " '1jjc4qfpeeffirv6pnq9aulrlk2fgl3k',\n",
       " 'dnmht6eebqauwh4w6akiy9q1zb0ebl0g',\n",
       " '8vp74g6kssomu1a6akix6y3hqy6552t7',\n",
       " 'ltl8gf0qeokew9j5fjouzjgj1iggwcqk',\n",
       " 'km9ndw8j2s65iwp2wmuys4izxnjrrzg3',\n",
       " 'z5ph5cupbgi6fqg4oxrkqkpbazwad4zy',\n",
       " '0ibi364m7i7l01xi4xqafyathrmrrnll',\n",
       " '2ez7hg33icitc4ycy86osihmyoaf6zmv',\n",
       " 'x84f5xk86kbqh76f1s5hg0dqob1y3hgj',\n",
       " 'rq9rbjhwbwlet1638neumftjn18nw6k6',\n",
       " 'gnubge5he0scnti43c95ylppja5hyu3e',\n",
       " 'epebfko9ccwvfzblh4jkeuo6m89ruv96',\n",
       " 'j6wo3pt6m38ptcky7otksvpxwfmjm2sy',\n",
       " '9cxhzri6m0ogecxhe1303rydgp9x67lw',\n",
       " 'ihpvfrf9xyc9zd4w5o1s27fgykwurh7k',\n",
       " '4t3jsm3ghdjuqa2b96xvgt5jnqf083qr',\n",
       " 'okq4csptd950rc2qghvg2noom96dhwsg',\n",
       " 'cc2kszsnpboklenkmf0e3am8ckusouqu',\n",
       " 'j61muejrwpt5t0q52gliep5h8nu0oqds',\n",
       " 'mn1ssilt7awub2bs4ign0wc8v54x3dz4',\n",
       " 'zo3dy2tmqdzyowk6d0amswrlsr4ze1pn',\n",
       " 'g0ibdptztphdny4qdkmkvlo78etktkuo',\n",
       " '56umhe8ybfqo3udonow6e7h9nh6zfcn9',\n",
       " '6d3syqencbejb4dzscqhbon4bn4t4fkc',\n",
       " 'zzsafm5u4tzk6k0ba500tlggn7iw8v03',\n",
       " 'veiwupaigla1qbijyxu3oujyjumg7wio',\n",
       " 'h0icm6kusqcb4tajgiafnkw6490426e9',\n",
       " 'bz0s8q7ylgzl29f87ma7hoj28qzrnvzj',\n",
       " 'v5dg3o7vfktpny3cuapf83hhn1xs9860',\n",
       " 'jp4ome1kafz9wl7j046j24mtn1y7br6z',\n",
       " 't6tbqb9o35p43lurms1gvwajkx28yni9',\n",
       " 'mvjrx5w1ykzs4xhfyp0u3isaotao9ig6',\n",
       " 'snilxp4fgzi281ayieikiwle0xkb1uok',\n",
       " '6rafm58i0f3qq9q646staz263l7zx23z',\n",
       " 'bq7ikhzq7xt66si1kmprz9vdjpnl5eqp',\n",
       " 'fav48xfxollnr5ho5pf5e2489ts2vhll',\n",
       " 'xjq9ej55dl8nthsehd6bye3cenlm32kq',\n",
       " '4h7ugot0byxla1yk0nf9kg9nxyf1hlli',\n",
       " 'ffw4du7oj59dgi5cobxa4exkvgj5l40y',\n",
       " 'lh00crko7a3iom3im9i4cywgc9m7vic3',\n",
       " 'w0payh6v0jjxkkafl6vsvc4n8kgpv7ox',\n",
       " 'f30qhm1a8mni00tmrtpmr9m06vdc4m3c',\n",
       " 'tbq8amnpqaruwlk4k59y24o88v2lrijv',\n",
       " '8l2kwf77tt7yyhd6ggu4n3diyhditcyg',\n",
       " 'sypliy4lb18jqrcikel4lwcq1f90lff8',\n",
       " 'orkj1m3ty84j2j44evyl475fqlyewkqx',\n",
       " '4ucsl52ybfm7q4altzes5m4yowvuu3md',\n",
       " 'wbxlv3oiicvb6obqrpc0ytxpfh7h0v0d',\n",
       " 'v4a5crla11s25l4qh3q6ttxtmdpqacak',\n",
       " '6cwjjqgf7iu88wyhp1e6c6gvmwilsuir',\n",
       " '11d9lz40vkh06i6tp2mubr1qbohm8biv',\n",
       " '4swgb6eg121uk3sbwxo149vxfmp9ithg',\n",
       " 'mtksp40840d55e7vqpq840bb8st06g9d',\n",
       " 'ynms1hvhmzagkbx86y4wp6nocxx1ktf2',\n",
       " 'gemo5q5llsk44vezvt3rztv0yyfgjaxf',\n",
       " 's01t3oa2x0f1228wsm3vrx08v1v7i34g',\n",
       " '23e4gafa4tl3zgd69vzt24sljwg7hium',\n",
       " '36o2vt1v0w4iwd29exnbgi27zi6au5fc',\n",
       " 'rmlt7geo1kvllujba7ssj9mweh8zehdh',\n",
       " 'a8w5juro2x4fbxo7bnwztfz0i8e61k9k',\n",
       " '2prnfocp00m0orj3ewj9ax9upu7qo4or',\n",
       " 'e9zdi89gp7vwfazymtxpngr8u5drtjzw',\n",
       " 'ypfbkkx6ffwmr9jiict56hl7zujikesq',\n",
       " 'qps6x3223x44a0f43wls21do54kvwlcg',\n",
       " 'a2is3y4a06exppvypo27imxbo2bt0nc8',\n",
       " '1s9ak9epkejejdgy99kuix4gqbxpjrf3',\n",
       " 'fvqxvfy7e7seko7xrgl60217yvk1xkh7',\n",
       " 'r9yxwyetyg7rqjwqcxx39lk5zrj0xlb7',\n",
       " 'e19lbjlbcx9q8785rhh8jy3gobpr30bf',\n",
       " 'txx1g7ne2p633i81z15w7lqz8p858pil']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GET K-FOLD SPLITS -----------------------------------------------------\n",
    "\n",
    "crossVal_splits = baseTrain.cross_val(_K_FOLD_CV_, _NUM_AUTHORS_)\n",
    "\n",
    "crossVal_splits[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df486efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GET TWEETS -----------------------------------------------------\n",
    "\n",
    "baseTrain.get_all_data(_TWEET_BATCH_SIZE_, tokenizer, _MAX_SEQ_LEN_, _PREPROCESS_TEXT_, NLI=True, label_hyp=label_hyp, nli_label2id=nli_label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25db26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d58a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aade6b3",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc026b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.DataLoaders import DatasetPANnli\n",
    "\n",
    "baseTest.get_all_data(_TWEET_BATCH_SIZE_, tokenizer, _MAX_SEQ_LEN_, _PREPROCESS_TEXT_, NLI=True, label_hyp=label_hyp, nli_label2id=nli_label2id)\n",
    "\n",
    "Test = DatasetPANnli(baseTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "925ba4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "samples = 4 * _NUM_AUTHORS_ * int(100 / _TWEET_BATCH_SIZE_)\n",
    "_LOGGING_STEPS_ = int(samples / _BATCH_SIZE_)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate               = _LEARNING_RATE_,\n",
    "    num_train_epochs            = _EPOCHS_,\n",
    "    per_device_train_batch_size = _BATCH_SIZE_,\n",
    "    per_device_eval_batch_size  = 200,\n",
    "    logging_steps               = _LOGGING_STEPS_,\n",
    "    output_dir                  = _OUTPUT_DIR_,\n",
    "    save_total_limit            = 5,\n",
    "    overwrite_output_dir        = True,\n",
    "    remove_unused_columns       = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa5522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing BartAdapterModel: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "- This IS expected if you are initializing BartAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9600\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5515' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5515/12000 20:53 < 24:34, 4.40 it/s, Epoch 4.59/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>0.563900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-500/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-500/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-500/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-500/fakeNews/pytorch_model_head.bin\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-1000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1000/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1000/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1000/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1000/fakeNews/pytorch_model_head.bin\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-1500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1500/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1500/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1500/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1500/fakeNews/pytorch_model_head.bin\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-2000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2000/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2000/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2000/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2000/fakeNews/pytorch_model_head.bin\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-2500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2500/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2500/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2500/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2500/fakeNews/pytorch_model_head.bin\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-3000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3000/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3000/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3000/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3000/fakeNews/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-3500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3500/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3500/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3500/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3500/fakeNews/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-4000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4000/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4000/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4000/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4000/fakeNews/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-4500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4500/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4500/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4500/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4500/fakeNews/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-5000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5000/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5000/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5000/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5000/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5000/fakeNews/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-5500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5500/fakeNews/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5500/fakeNews/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5500/fakeNews/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5500/fakeNews/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5500/fakeNews/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-3000] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoAdapterModel, AutoModelForSequenceClassification\n",
    "from tools.DataLoaders import DatasetCrossValnli\n",
    "from transformers import AdapterTrainer\n",
    "from tools.Testing import compute_author_predictions_nli, compute_author_predictions_nli_LR\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "task = label_name\n",
    "\n",
    "f1s_soft = []\n",
    "f1s_hard = []\n",
    "\n",
    "f1s_soft_LR = []\n",
    "f1s_hard_LR = []\n",
    "\n",
    "for split in range( _K_FOLD_CV_ ):\n",
    "    \n",
    "    # loaders for current split ------------------------------------------\n",
    "    \n",
    "    authors_train, authors_val = crossVal_splits[split]\n",
    "    \n",
    "    Train = DatasetCrossValnli(baseTrain, authors_train)\n",
    "    Val   = DatasetCrossValnli(baseTrain, authors_val)\n",
    "    \n",
    "    \n",
    "    # initialize model ---------------------------------------------------\n",
    "    \n",
    "\n",
    "    #static_head_model = AutoModelForSequenceClassification.from_pretrained(_PRETRAINED_LM_)\n",
    "    #static_head_model.add_adapter(task, config = _ADAPTER_CONFIG_) \n",
    "    #static_head_model.save_adapter('aux_adapter_'+_PRED_DIR_, task)\n",
    "\n",
    "    model = AutoAdapterModel.from_pretrained(_PRETRAINED_LM_)\n",
    "    #model.load_adapter('aux_adapter_'+_PRED_DIR_, load_as=task)\n",
    "    model.load_adapter('Pretrained_Adapters_NLI/'+_PRETRAINED_ADAPTER_, load_as=task)\n",
    "\n",
    "    model.set_active_adapters(task)\n",
    "    model.train_adapter(task)\n",
    "\n",
    "\n",
    "    # create trainer and train -------------------------------------------\n",
    "\n",
    "    trainer = AdapterTrainer(\n",
    "        model           = model,\n",
    "        args            = training_args,\n",
    "        train_dataset   = Train,\n",
    "    )\n",
    "    trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "    # get predictions ----------------------------------------------------\n",
    "    \n",
    "    \n",
    "    ignore_keys = None\n",
    "    if is_encoder_decoder:\n",
    "        ignore_keys = ['encoder_last_hidden_state']\n",
    "\n",
    "    results            = trainer.predict(Test , ignore_keys = ignore_keys)\n",
    "    author_predictions = compute_author_predictions_nli(baseTest, results.predictions, task, len(class_dict), nli_label2id)\n",
    "\n",
    "    # report metrics \n",
    "\n",
    "    report = {'soft': classification_report(author_predictions['true'], author_predictions['pred_soft'], digits=4), \n",
    "               'hard': classification_report(author_predictions['true'], author_predictions['pred_hard'], digits=4)}\n",
    "\n",
    "    f1s_soft.append( f1_score(author_predictions['true'], author_predictions['pred_soft'], average = 'macro') )\n",
    "    f1s_hard.append( f1_score(author_predictions['true'], author_predictions['pred_hard'], average = 'macro') )\n",
    "    \n",
    "    print(\"Results with split \" + str(split + 1) + \":\\n\")\n",
    "    print(\"soft voting:\\n\", report['soft'], '\\n')\n",
    "    print(\"hard voting:\\n\", report['hard'])\n",
    "\n",
    "\n",
    "    # get predictions with Logistic Regression----------------------------\n",
    "\n",
    "    resultsTrain = trainer.predict(Train, ignore_keys = ignore_keys)\n",
    "    author_predictions_LR = compute_author_predictions_nli_LR(Train, baseTest, resultsTrain.predictions, results.predictions, task, len(class_dict))\n",
    "    \n",
    "    f1s_soft_LR.append( f1_score(author_predictions_LR['true'], author_predictions_LR['pred_soft'], average = 'macro') )\n",
    "    f1s_hard_LR.append( f1_score(author_predictions_LR['true'], author_predictions_LR['pred_hard'], average = 'macro') )\n",
    "    \n",
    "    # report metrics \n",
    "\n",
    "    report_LR = {'soft': classification_report(author_predictions_LR['true'], author_predictions_LR['pred_soft'], digits=4), \n",
    "               'hard': classification_report(author_predictions_LR['true'], author_predictions_LR['pred_hard'], digits=4)}\n",
    "\n",
    "    print(\"Results with split \" + str(split + 1) + \" using LOGISTIC REGRESSION:\\n\")\n",
    "    print(\"soft voting:\\n\", report_LR['soft'], '\\n')\n",
    "    print(\"hard voting:\\n\", report_LR['hard'])\n",
    "\n",
    "     \n",
    "    \n",
    "    # save predictions ----------------------------------------------------\n",
    "    \n",
    "    DIR = 'results/' + _DATASET_ + '/' + _LANGUAGE_ + '/' + _PRED_DIR_ + '/' + str(_NUM_AUTHORS_) + '_authors/test_split_' + str(split + 1) + '/'\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    with open(DIR + 'predictions.pickle', 'wb') as f:\n",
    "        pickle.dump(author_predictions, f)\n",
    "    \n",
    "    with open(DIR + 'predictions_LR.pickle', 'wb') as f:\n",
    "        pickle.dump(author_predictions_LR, f)\n",
    "\n",
    "    with open(DIR + 'report.txt', 'w') as f:\n",
    "        f.write(\"soft voting:\\n\" + report['soft'] + '\\n\\n')\n",
    "        f.write(\"hard voting:\\n\" + report['hard'])\n",
    "        \n",
    "    with open(DIR + 'report_LR.txt', 'w') as f:\n",
    "        f.write(\"soft voting:\\n\" + report_LR['soft'] + '\\n\\n')\n",
    "        f.write(\"hard voting:\\n\" + report_LR['hard'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59550724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft results:  [0.724442   0.73989596 0.7        0.71499287 0.74      ]\n",
      "\n",
      "Hard results:  [0.71929825 0.73       0.69997    0.72493811 0.73499337]\n",
      "\n",
      "\n",
      "Soft statistics: \n",
      "\t[avg, std]: [0.7238661656490359, 0.0152703346286036]\n",
      "\n",
      "Hard statistics: \n",
      "\t[avg, std]: [0.7218399457046195, 0.012115714657777396]\n"
     ]
    }
   ],
   "source": [
    "# report statistics\n",
    "\n",
    "print('Soft results: ', f1s_soft)\n",
    "print('\\nHard results: ', f1s_hard)\n",
    "\n",
    "f1s_soft = np.array(f1s_soft)\n",
    "f1s_hard = np.array(f1s_hard)\n",
    "\n",
    "FewShot_Results = {'soft': [f1s_soft.mean(), f1s_soft.std()], 'hard': [f1s_hard.mean(), f1s_hard.std()]}\n",
    "\n",
    "print('\\n\\nSoft statistics: ')\n",
    "print('\\t[avg, std]:', FewShot_Results['soft'])\n",
    "\n",
    "print('\\nHard statistics: ')\n",
    "print('\\t[avg, std]:', FewShot_Results['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f42fe7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft results:  [0.73989596 0.73307144 0.71929825 0.70953526 0.72867049]\n",
      "\n",
      "Hard results:  [0.73       0.70895223 0.69987995 0.70953526 0.71929825]\n",
      "\n",
      "\n",
      "Soft statistics with LOGISTIC REGRESSION: \n",
      "\t[avg, std]: [0.7260942773896794, 0.010641346567572449]\n",
      "\n",
      "Hard statistics with LOGISTIC REGRESSION: \n",
      "\t[avg, std]: [0.7135331364051918, 0.010273960208663223]\n"
     ]
    }
   ],
   "source": [
    "# report statistics\n",
    "\n",
    "print('Soft results: ', f1s_soft_LR)\n",
    "print('\\nHard results: ', f1s_hard_LR)\n",
    "\n",
    "f1s_soft_LR = np.array(f1s_soft_LR)\n",
    "f1s_hard_LR = np.array(f1s_hard_LR)\n",
    "\n",
    "FewShot_Results_LR = {'soft': [f1s_soft_LR.mean(), f1s_soft_LR.std()], 'hard': [f1s_hard_LR.mean(), f1s_hard_LR.std()]}\n",
    "\n",
    "print('\\n\\nSoft statistics with LOGISTIC REGRESSION: ')\n",
    "print('\\t[avg, std]:', FewShot_Results_LR['soft'])\n",
    "\n",
    "print('\\nHard statistics with LOGISTIC REGRESSION: ')\n",
    "print('\\t[avg, std]:', FewShot_Results_LR['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa14dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e598acda",
   "metadata": {},
   "source": [
    "## Training with all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc15e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 96000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.DataLoaders import DatasetPANnli\n",
    "\n",
    "baseTest.get_all_data(_TWEET_BATCH_SIZE_, tokenizer, _MAX_SEQ_LEN_, _PREPROCESS_TEXT_, NLI=True, label_hyp=label_hyp, nli_label2id=nli_label2id)\n",
    "\n",
    "Test  = DatasetPANnli(baseTest)\n",
    "Train = DatasetPANnli(baseTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66c7ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "samples = 4 * _NUM_AUTHORS_ * int(100 / _TWEET_BATCH_SIZE_)\n",
    "_LOGGING_STEPS_ = int(samples / _BATCH_SIZE_)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate               = _LEARNING_RATE_,\n",
    "    num_train_epochs            = _EPOCHS_,\n",
    "    per_device_train_batch_size = _BATCH_SIZE_,\n",
    "    per_device_eval_batch_size  = 200,\n",
    "    logging_steps               = _LOGGING_STEPS_,\n",
    "    output_dir                  = _OUTPUT_DIR_,\n",
    "    save_total_limit            = 5,\n",
    "    overwrite_output_dir        = True,\n",
    "    remove_unused_columns       = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340ae862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing BartAdapterModel: ['classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight', 'classification_head.dense.bias']\n",
      "- This IS expected if you are initializing BartAdapterModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartAdapterModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 144000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 180000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6027' max='180000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6027/180000 22:42 < 10:55:42, 4.42 it/s, Epoch 0.33/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>0.669700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-500/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-500/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-500/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-500/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-204000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-1000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1000/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1000/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1000/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1000/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-204500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-1500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1500/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1500/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1500/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-1500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-1500/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-205000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-2000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2000/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2000/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2000/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2000/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-205500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-2500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2500/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2500/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2500/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-2500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-2500/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-206000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-3000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3000/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3000/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3000/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3000/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-3500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3500/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3500/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3500/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-3500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-3500/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-4000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4000/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4000/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4000/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4000/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-4500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4500/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4500/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4500/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-4500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-4500/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-5000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5000/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5000/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5000/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5000/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-5500\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5500/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5500/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5500/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-5500/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-5500/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsParallelNLI/checkpoint-6000\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-6000/gender/adapter_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-6000/gender/pytorch_adapter.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-6000/gender/head_config.json\n",
      "Module weights saved in checkPointsParallelNLI/checkpoint-6000/gender/pytorch_model_head.bin\n",
      "Configuration saved in checkPointsParallelNLI/checkpoint-6000/gender/head_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Module weights saved in checkPointsParallelNLI/checkpoint-6000/gender/pytorch_model_head.bin\n",
      "Deleting older checkpoint [checkPointsParallelNLI/checkpoint-3500] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoAdapterModel, AutoModelForSequenceClassification\n",
    "from tools.DataLoaders import DatasetCrossValnli\n",
    "from transformers import AdapterTrainer\n",
    "from tools.Testing import compute_author_predictions_nli, compute_author_predictions_nli_LR\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "task = label_name\n",
    "\n",
    "# initialize model ---------------------------------------------------\n",
    "\n",
    "static_head_model = AutoModelForSequenceClassification.from_pretrained(_PRETRAINED_LM_)\n",
    "static_head_model.add_adapter(task, config = _ADAPTER_CONFIG_) \n",
    "static_head_model.save_adapter('aux_adapter_'+_PRED_DIR_, task)\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(_PRETRAINED_LM_)\n",
    "model.load_adapter('aux_adapter_'+_PRED_DIR_, load_as=task)\n",
    "#model.load_adapter('pretrained_adapter_NLI', load_as=task)\n",
    "\n",
    "model.set_active_adapters(task)\n",
    "model.train_adapter(task)\n",
    "\n",
    "\n",
    "# create trainer and train -------------------------------------------\n",
    "\n",
    "trainer = AdapterTrainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = Train,\n",
    ")\n",
    "trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# get predictions ----------------------------------------------------\n",
    "\n",
    "\n",
    "ignore_keys = None\n",
    "if is_encoder_decoder:\n",
    "    ignore_keys = ['encoder_last_hidden_state']\n",
    "\n",
    "results            = trainer.predict(Test , ignore_keys = ignore_keys)\n",
    "author_predictions = compute_author_predictions_nli(baseTest, results.predictions, task, len(class_dict), nli_label2id)\n",
    "\n",
    "# report metrics \n",
    "\n",
    "report = {'soft': classification_report(author_predictions['true'], author_predictions['pred_soft'], digits=4), \n",
    "           'hard': classification_report(author_predictions['true'], author_predictions['pred_hard'], digits=4)}\n",
    "\n",
    "print(\"Results:\\n\")\n",
    "print(\"soft voting:\\n\", report['soft'], '\\n')\n",
    "print(\"hard voting:\\n\", report['hard'])\n",
    "\n",
    "\n",
    "# get predictions with Logistic Regression----------------------------\n",
    "\n",
    "\n",
    "resultsTrain = trainer.predict(Train, ignore_keys = ignore_keys)\n",
    "author_predictions_LR = compute_author_predictions_nli_LR(baseTrain, baseTest, resultsTrain.predictions, results.predictions, task, len(class_dict))\n",
    "\n",
    "# report metrics \n",
    "\n",
    "report_LR = {'soft': classification_report(author_predictions_LR['true'], author_predictions_LR['pred_soft'], digits=4), \n",
    "           'hard': classification_report(author_predictions_LR['true'], author_predictions_LR['pred_hard'], digits=4)}\n",
    "\n",
    "print(\"Results using LOGISTIC REGRESSION:\\n\")\n",
    "print(\"soft voting:\\n\", report_LR['soft'], '\\n')\n",
    "print(\"hard voting:\\n\", report_LR['hard'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dc3b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7400    0.7400    0.7400       100\n",
      "           1     0.7400    0.7400    0.7400       100\n",
      "\n",
      "    accuracy                         0.7400       200\n",
      "   macro avg     0.7400    0.7400    0.7400       200\n",
      "weighted avg     0.7400    0.7400    0.7400       200\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7374    0.7300    0.7337       100\n",
      "           1     0.7327    0.7400    0.7363       100\n",
      "\n",
      "    accuracy                         0.7350       200\n",
      "   macro avg     0.7350    0.7350    0.7350       200\n",
      "weighted avg     0.7350    0.7350    0.7350       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Results:\\n\")\n",
    "print(\"soft voting:\\n\", report['soft'], '\\n')\n",
    "print(\"hard voting:\\n\", report['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "021038ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results using LOGISTIC REGRESSION:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7674    0.6600    0.7097       100\n",
      "           1     0.7018    0.8000    0.7477       100\n",
      "\n",
      "    accuracy                         0.7300       200\n",
      "   macro avg     0.7346    0.7300    0.7287       200\n",
      "weighted avg     0.7346    0.7300    0.7287       200\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7444    0.6700    0.7053       100\n",
      "           1     0.7000    0.7700    0.7333       100\n",
      "\n",
      "    accuracy                         0.7200       200\n",
      "   macro avg     0.7222    0.7200    0.7193       200\n",
      "weighted avg     0.7222    0.7200    0.7193       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Results using LOGISTIC REGRESSION:\\n\")\n",
    "print(\"soft voting:\\n\", report_LR['soft'], '\\n')\n",
    "print(\"hard voting:\\n\", report_LR['hard'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b38a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbdfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_adapter('Pretrained_Adapters_NLI/'+_DATASET_+'_'+_PRED_DIR_, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e13cc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
