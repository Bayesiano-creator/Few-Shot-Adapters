{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5287a8d",
   "metadata": {},
   "source": [
    "## Set Global Seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e68a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The global seed 260615\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = 260615\n",
    "set_all_seeds(seed)\n",
    "\n",
    "print(\"The global seed \" + str(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a407e6",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0faaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGUAGE\n",
    "\n",
    "_LANGUAGE_         = 'es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f81690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CLASSIFICATION\n",
    "\n",
    "_PRETRAINED_LM_    = 'pysentimiento/robertuito-base-uncased'\n",
    "_PREPROCESS_TEXT_  = True\n",
    "_TWEET_BATCH_SIZE_ = 5\n",
    "_ADAPTER_CONFIG_   = transformers.ParallelConfig(reduction_factor = 256)\n",
    "_MAX_SEQ_LEN_      = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daabdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "_OUTPUT_DIR_       = 'checkPointsFFT'\n",
    "_LOGGING_STEPS_    = 50\n",
    "_NUM_AUTHORS_      = 8\n",
    "_K_FOLD_CV_        = 5\n",
    "_NO_GPUS_          = 1\n",
    "_BATCH_SIZE_       = int(32 / _NO_GPUS_)\n",
    "_EPOCHS_           = 10\n",
    "_LEARNING_RATE_    = 1e-5\n",
    "\n",
    "# PREDICTIONS\n",
    "\n",
    "_DATASET_          = 'PAN17'\n",
    "_PRED_DIR_         = 'FFT_5tweet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c9214",
   "metadata": {},
   "source": [
    "## Other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b722fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DICTONARIES -----------------------------------------------------------------------\n",
    "\n",
    "gender_dict    = {'female': 0, 'male':   1}\n",
    "varietyEN_dict = {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}\n",
    "varietyES_dict = {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}  \n",
    "\n",
    "genderEN_hip  = {0: 'I’m a female', 1: 'I’m a male'}\n",
    "genderES_hip  = {0: 'Mi nombre es María', 1: 'Mi nombre es José'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc921f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LANGUAGE DICTIONARIES\n",
    "\n",
    "if _LANGUAGE_ == 'en':\n",
    "    gender_hip   = genderEN_hip\n",
    "    variety_dict = varietyEN_dict\n",
    "\n",
    "elif _LANGUAGE_ == 'es':\n",
    "    gender_hip   = genderES_hip\n",
    "    variety_dict = varietyES_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c35894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LANGUAGE TOKENIZER\n",
    "\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(_PRETRAINED_LM_)\n",
    "vocab = tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef9838",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6438e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 419998\n",
      "\n",
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 280000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.DataLoaders import BasePAN17\n",
    "\n",
    "baseTrain  = BasePAN17(Dir             = 'data/2017',\n",
    "                      split            = 'train',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = 1,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_)\n",
    "\n",
    "baseTest  = BasePAN17(Dir              = 'data/2017',\n",
    "                      split            = 'test',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = 1,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6a42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossVal_splits = []\n",
    "\n",
    "for val_idx in range(_K_FOLD_CV_):\n",
    "    \n",
    "    authors_train, authors_val = baseTrain.cross_val(_K_FOLD_CV_, val_idx, _NUM_AUTHORS_)\n",
    "    \n",
    "    crossVal_splits.append( (authors_train, authors_val) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad88695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 84000\n",
      "\n",
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 56000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.DataLoaders import BasePAN17\n",
    "\n",
    "baseTrain  = BasePAN17(Dir             = 'data/2017',\n",
    "                      split            = 'train',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_)\n",
    "\n",
    "baseTest  = BasePAN17(Dir              = 'data/2017',\n",
    "                      split            = 'test',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2a37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.DataLoaders import DatasetPAN17\n",
    "\n",
    "Test = DatasetPAN17(baseTest, 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49820915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bafc7670beae172539e66c48858d8a',\n",
       " '70250236e5aae9df16d3d081b24f8c52',\n",
       " '4230dfe217cc7d667e6a21bff301263d',\n",
       " 'b1be8bd39d7a3be53d2e37a654db05d8',\n",
       " '8690c40c4fea5fdb6ad1c030cafca7f',\n",
       " 'eb3d30f6a613d63c495faf276ce8af20',\n",
       " 'aeef24da173bad8c8589593068d12e2b',\n",
       " '3fccf54ed8ff8ebf3e09b27178c3eda6',\n",
       " 'bca70cfd674db9f5e0885728d43043e7',\n",
       " 'cdca31bfd55d33ef52918fbd1b4e16cc',\n",
       " '4e0f3dc3fa3a35480d7a27bf12816ae1',\n",
       " 'bad426ef9226de53f904ee17f813abb1',\n",
       " '921ec4a999f5b8c26f2102e6a6fd2e26',\n",
       " '6b8c65a63a47a179aa0729944df6c16d',\n",
       " 'df3eaaf1eadbc7de0f8c496a4d5adf0b',\n",
       " 'a216de7f51f7022889e2eb86e48155d8']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossVal_splits[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a3700",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bfe625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "samples = 2 * _NUM_AUTHORS_ * int(100 / _TWEET_BATCH_SIZE_)\n",
    "_LOGGING_STEPS_ = int(samples / _BATCH_SIZE_)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate               = _LEARNING_RATE_,\n",
    "    num_train_epochs            = _EPOCHS_,\n",
    "    per_device_train_batch_size = _BATCH_SIZE_,\n",
    "    per_device_eval_batch_size  = 200,\n",
    "    logging_steps               = _LOGGING_STEPS_,\n",
    "    output_dir                  = _OUTPUT_DIR_,\n",
    "    save_total_limit            = 10,\n",
    "    overwrite_output_dir        = True,\n",
    "    remove_unused_columns       = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "716df656",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.653600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.151600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 840/840 [00:04<00:00, 199.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 1:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6489    0.5500    0.5954       420\n",
      "           1     0.6095    0.7024    0.6527       420\n",
      "\n",
      "    accuracy                         0.6262       840\n",
      "   macro avg     0.6292    0.6262    0.6240       840\n",
      "weighted avg     0.6292    0.6262    0.6240       840\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6364    0.5500    0.5900       420\n",
      "           1     0.6038    0.6857    0.6421       420\n",
      "\n",
      "    accuracy                         0.6179       840\n",
      "   macro avg     0.6201    0.6179    0.6161       840\n",
      "weighted avg     0.6201    0.6179    0.6161       840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/5212cb9b5b32726fce956daa9a21ee0a0c2b6e54c54d1af58c678217d85f8143.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/8b04e6b193dae308280a39e7414ec9e55aa85418ff297d0e509550de00b93630.273dce86eeb4b83137e2e5511e720f2949cafbe41d618f259e97749592c88fdb\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.636700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.395500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.149900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.132500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 840/840 [00:04<00:00, 201.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 2:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5318    0.6762    0.5954       420\n",
      "           1     0.5556    0.4048    0.4683       420\n",
      "\n",
      "    accuracy                         0.5405       840\n",
      "   macro avg     0.5437    0.5405    0.5319       840\n",
      "weighted avg     0.5437    0.5405    0.5319       840\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5353    0.6857    0.6013       420\n",
      "           1     0.5629    0.4048    0.4709       420\n",
      "\n",
      "    accuracy                         0.5452       840\n",
      "   macro avg     0.5491    0.5452    0.5361       840\n",
      "weighted avg     0.5491    0.5452    0.5361       840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/5212cb9b5b32726fce956daa9a21ee0a0c2b6e54c54d1af58c678217d85f8143.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/8b04e6b193dae308280a39e7414ec9e55aa85418ff297d0e509550de00b93630.273dce86eeb4b83137e2e5511e720f2949cafbe41d618f259e97749592c88fdb\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.569200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.505200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.312000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.291200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.283700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 840/840 [00:03<00:00, 222.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 3:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5412    0.4381    0.4842       420\n",
      "           1     0.5280    0.6286    0.5739       420\n",
      "\n",
      "    accuracy                         0.5333       840\n",
      "   macro avg     0.5346    0.5333    0.5291       840\n",
      "weighted avg     0.5346    0.5333    0.5291       840\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5417    0.4643    0.5000       420\n",
      "           1     0.5312    0.6071    0.5667       420\n",
      "\n",
      "    accuracy                         0.5357       840\n",
      "   macro avg     0.5365    0.5357    0.5333       840\n",
      "weighted avg     0.5365    0.5357    0.5333       840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/5212cb9b5b32726fce956daa9a21ee0a0c2b6e54c54d1af58c678217d85f8143.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/8b04e6b193dae308280a39e7414ec9e55aa85418ff297d0e509550de00b93630.273dce86eeb4b83137e2e5511e720f2949cafbe41d618f259e97749592c88fdb\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.579800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.444700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.285100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.275700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 840/840 [00:03<00:00, 276.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 4:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5582    0.6619    0.6057       420\n",
      "           1     0.5848    0.4762    0.5249       420\n",
      "\n",
      "    accuracy                         0.5690       840\n",
      "   macro avg     0.5715    0.5690    0.5653       840\n",
      "weighted avg     0.5715    0.5690    0.5653       840\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5580    0.6762    0.6114       420\n",
      "           1     0.5891    0.4643    0.5193       420\n",
      "\n",
      "    accuracy                         0.5702       840\n",
      "   macro avg     0.5735    0.5702    0.5654       840\n",
      "weighted avg     0.5735    0.5702    0.5654       840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/5212cb9b5b32726fce956daa9a21ee0a0c2b6e54c54d1af58c678217d85f8143.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/8b04e6b193dae308280a39e7414ec9e55aa85418ff297d0e509550de00b93630.273dce86eeb4b83137e2e5511e720f2949cafbe41d618f259e97749592c88fdb\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 320\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.657800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.560900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.338300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.257900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 16800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████| 840/840 [00:04<00:00, 191.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 5:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5964    0.7143    0.6501       420\n",
      "           1     0.6439    0.5167    0.5733       420\n",
      "\n",
      "    accuracy                         0.6155       840\n",
      "   macro avg     0.6202    0.6155    0.6117       840\n",
      "weighted avg     0.6202    0.6155    0.6117       840\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5872    0.7214    0.6474       420\n",
      "           1     0.6389    0.4929    0.5565       420\n",
      "\n",
      "    accuracy                         0.6071       840\n",
      "   macro avg     0.6130    0.6071    0.6019       840\n",
      "weighted avg     0.6130    0.6071    0.6019       840\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from tools.DataLoaders import DatasetCrossVal\n",
    "from transformers import Trainer\n",
    "from tools.Testing import compute_author_predictions\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "task = 'gender'\n",
    "\n",
    "f1s_soft = []\n",
    "f1s_hard = []\n",
    "\n",
    "for split in range( _K_FOLD_CV_ ):\n",
    "    \n",
    "    # loaders for current split ------------------------------------------\n",
    "    \n",
    "    authors_train, authors_val = crossVal_splits[split]\n",
    "    \n",
    "    Train = DatasetCrossVal(baseTrain, authors_train, task)\n",
    "    Val   = DatasetCrossVal(baseTrain, authors_val  , task)\n",
    "    \n",
    "    \n",
    "    # initialize model ---------------------------------------------------\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(_PRETRAINED_LM_, num_labels = 2)\n",
    "    \n",
    "    \n",
    "    # create trainer and train -------------------------------------------\n",
    "        \n",
    "    trainer = Trainer(\n",
    "        model           = model,\n",
    "        args            = training_args,\n",
    "        train_dataset   = Train,\n",
    "    )\n",
    "    trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "    # get predictions ----------------------------------------------------\n",
    "    \n",
    "    results            = trainer.predict(Val)\n",
    "    author_predictions = compute_author_predictions(Val, results.predictions, 'gender', 2)\n",
    "    \n",
    "    \n",
    "    # report metrics -----------------------------------------------------\n",
    "    \n",
    "    report = {'soft': classification_report(author_predictions['true'], author_predictions['pred_soft'], digits=4), \n",
    "               'hard': classification_report(author_predictions['true'], author_predictions['pred_hard'], digits=4)}\n",
    "\n",
    "    f1s_soft.append( f1_score(author_predictions['true'], author_predictions['pred_soft'], average = 'macro') )\n",
    "    f1s_hard.append( f1_score(author_predictions['true'], author_predictions['pred_hard'], average = 'macro') )\n",
    "\n",
    "    print(\"Results with split \" + str(split + 1) + \":\\n\")\n",
    "    print(\"soft voting:\\n\", report['soft'], '\\n')\n",
    "    print(\"hard voting:\\n\", report['hard'])\n",
    "     \n",
    "    \n",
    "    # save predictions ----------------------------------------------------\n",
    "    \n",
    "    DIR = 'results/' + _DATASET_ + '/' + _LANGUAGE_ + '/' + _PRED_DIR_ + '/' + str(_NUM_AUTHORS_) + '_authors/split_' + str(split + 1) + '/'\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    with open(DIR + 'predictions.pickle', 'wb') as f:\n",
    "        pickle.dump(author_predictions, f)\n",
    "\n",
    "    with open(DIR + 'report.txt', 'w') as f:\n",
    "        f.write(\"soft voting:\\n\" + report['soft'] + '\\n\\n')\n",
    "        f.write(\"hard voting:\\n\" + report['hard'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a56aa6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft results:  [0.6240078459994526, 0.5318536999497548, 0.5290617848970252, 0.5652994356097645, 0.6116849455640458]\n",
      "\n",
      "Hard results:  [0.616089391201835, 0.5360833685135815, 0.5333333333333333, 0.5653588541435244, 0.6019437551695617]\n",
      "\n",
      "\n",
      "Soft statistics: \n",
      "\t[avg, std]: [0.5723815424040086, 0.03944438201821679]\n",
      "\n",
      "Hard statistics: \n",
      "\t[avg, std]: [0.5705617404723672, 0.033643365719226874]\n"
     ]
    }
   ],
   "source": [
    "# report statistics\n",
    "\n",
    "print('Soft results: ', f1s_soft)\n",
    "print('\\nHard results: ', f1s_hard)\n",
    "\n",
    "f1s_soft = np.array(f1s_soft)\n",
    "f1s_hard = np.array(f1s_hard)\n",
    "\n",
    "FewShot_Results = {'soft': [f1s_soft.mean(), f1s_soft.std()], 'hard': [f1s_hard.mean(), f1s_hard.std()]}\n",
    "\n",
    "print('\\n\\nSoft statistics: ')\n",
    "print('\\t[avg, std]:', FewShot_Results['soft'])\n",
    "\n",
    "print('\\nHard statistics: ')\n",
    "print('\\t[avg, std]:', FewShot_Results['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dda12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d0e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7984a07",
   "metadata": {},
   "source": [
    "## Test the best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "491d18c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/5212cb9b5b32726fce956daa9a21ee0a0c2b6e54c54d1af58c678217d85f8143.4cce50d5a926bf18fe43f2ea8d4596b505e97a64e6e700e993def66b06f1c83b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"pysentimiento/robertuito-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/pysentimiento/robertuito-base-uncased/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/8b04e6b193dae308280a39e7414ec9e55aa85418ff297d0e509550de00b93630.273dce86eeb4b83137e2e5511e720f2949cafbe41d618f259e97749592c88fdb\n",
      "Some weights of the model checkpoint at pysentimiento/robertuito-base-uncased were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pysentimiento/robertuito-base-uncased and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 5120\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/1600 06:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.496200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.396200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.079200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsFFT/checkpoint-500\n",
      "Configuration saved in checkPointsFFT/checkpoint-500/config.json\n",
      "Model weights saved in checkPointsFFT/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to checkPointsFFT/checkpoint-1000\n",
      "Configuration saved in checkPointsFFT/checkpoint-1000/config.json\n",
      "Model weights saved in checkPointsFFT/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to checkPointsFFT/checkpoint-1500\n",
      "Configuration saved in checkPointsFFT/checkpoint-1500/config.json\n",
      "Model weights saved in checkPointsFFT/checkpoint-1500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 56000\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████| 2800/2800 [00:31<00:00, 89.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 2:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7855    0.7507    0.7677      1400\n",
      "           1     0.7613    0.7950    0.7778      1400\n",
      "\n",
      "    accuracy                         0.7729      2800\n",
      "   macro avg     0.7734    0.7729    0.7727      2800\n",
      "weighted avg     0.7734    0.7729    0.7727      2800\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7639    0.7650    0.7645      1400\n",
      "           1     0.7647    0.7636    0.7641      1400\n",
      "\n",
      "    accuracy                         0.7643      2800\n",
      "   macro avg     0.7643    0.7643    0.7643      2800\n",
      "weighted avg     0.7643    0.7643    0.7643      2800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from tools.DataLoaders import DatasetCrossVal, DatasetPAN17\n",
    "from transformers import Trainer\n",
    "from tools.Testing import compute_author_predictions\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "task = 'gender'\n",
    "\n",
    "split = 1\n",
    "\n",
    "# loaders for current split ------------------------------------------\n",
    "    \n",
    "authors_train, authors_val = crossVal_splits[split]\n",
    "\n",
    "Train = DatasetCrossVal(baseTrain, authors_train, task)\n",
    "Val   = DatasetCrossVal(baseTrain, authors_val  , task)\n",
    "\n",
    "\n",
    "# initialize model ---------------------------------------------------\n",
    "    \n",
    "model = AutoModelForSequenceClassification.from_pretrained(_PRETRAINED_LM_, num_labels = 2)\n",
    "\n",
    "\n",
    "# create trainer and train -------------------------------------------\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = Train,\n",
    ")\n",
    "trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# get predictions ----------------------------------------------------\n",
    "\n",
    "results            = trainer.predict(Test)\n",
    "author_predictions = compute_author_predictions(baseTest, results.predictions, 'gender', 2)\n",
    "\n",
    "\n",
    "# report metrics -----------------------------------------------------\n",
    "\n",
    "report = {'soft': classification_report(author_predictions['true'], author_predictions['pred_soft'], digits=4), \n",
    "           'hard': classification_report(author_predictions['true'], author_predictions['pred_hard'], digits=4)}\n",
    "\n",
    "print(\"Results with split \" + str(split + 1) + \":\\n\")\n",
    "print(\"soft voting:\\n\", report['soft'], '\\n')\n",
    "print(\"hard voting:\\n\", report['hard'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00349e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
