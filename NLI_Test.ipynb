{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7086ef4c",
   "metadata": {},
   "source": [
    "## Set Global Seed and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f11b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The global seed 559967\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = 559967#random.randint(0,1e6) \n",
    "set_all_seeds(seed)\n",
    "\n",
    "print(\"The global seed \" + str(seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84094236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "_LANGUAGE_         = 'en'\n",
    "_PRETRAINED_LM_    = 'ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli'\n",
    "_PREPROCESS_TEXT_  = True\n",
    "_TWEET_BATCH_SIZE_ = 2\n",
    "_ADAPTER_CONFIG_   = transformers.ParallelConfig(reduction_factor = 256)\n",
    "_MAX_SEQ_LEN_      = 150\n",
    "\n",
    "# TRAIN\n",
    "\n",
    "_OUTPUT_DIR_       = 'checkPoints_FT'\n",
    "_LOGGING_STEPS_    = 2\n",
    "_NUM_AUTHORS_      = [8]\n",
    "_K_FOLD_CV_        = 5\n",
    "_NO_GPUS_          = 1\n",
    "_BATCH_SIZE_       = int(32 / _NO_GPUS_)\n",
    "_EPOCHS_           = {'gender': 30, 'variety': 30}\n",
    "_LEARNING_RATE_    = 1e-5\n",
    "\n",
    "# PREDICTIONS\n",
    "_DATASET_          = 'PAN17'\n",
    "_PRED_DIR_         = 'gender_FT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24462750",
   "metadata": {},
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f577ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DICTONARIES -----------------------------------------------------------------------\n",
    "\n",
    "gender_dict    = {'female': 0, 'male':   1}\n",
    "varietyEN_dict = {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}\n",
    "varietyES_dict = {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}  \n",
    "\n",
    "genderEN_hip  = {0: 'I’m a female', 1: 'I’m a male'}\n",
    "genderES_hip  = {0: 'Mi nombre es María', 1: 'Mi nombre es José'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f700b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LANGUAGE DICTIONARY\n",
    "\n",
    "if _LANGUAGE_ == 'en':\n",
    "    gender_hip   = genderEN_hip\n",
    "    variety_dict = varietyEN_dict\n",
    "\n",
    "elif _LANGUAGE_ == 'es':\n",
    "    gender_hip   = genderES_hip\n",
    "    variety_dict = varietyES_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f39f6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bart to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# SET LANGUAGE TOKENIZER\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(_PRETRAINED_LM_)\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "config             = PretrainedConfig.from_pretrained(_PRETRAINED_LM_)\n",
    "nli_label2id       = config.label2id\n",
    "is_encoder_decoder = config.is_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e509445d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734e5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.DataLoaders import BasePAN17nli, DatasetPAN17nli, DatasetCrossValnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38c0b26a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 240000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseTest  = BasePAN17nli(Dir           = 'data/2017',\n",
    "                      split            = 'test',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_,\n",
    "                      label            = 'gender',\n",
    "                      label_hip        = gender_hip,\n",
    "                      nli_label2id     = nli_label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "455b47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Test  = DatasetPAN17nli(Base_Dataset = baseTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8dd14",
   "metadata": {},
   "source": [
    "## Compute predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a4077d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92265644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 240000\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoAdapterModel\n",
    "from transformers import TrainingArguments, Trainer, AdapterTrainer, EarlyStoppingCallback\n",
    "from tools.Testing import compute_accuracy, compute_author_predictions\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(_PRETRAINED_LM_)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate               = _LEARNING_RATE_,\n",
    "    num_train_epochs            = _EPOCHS_[tasks[0]],\n",
    "    per_device_train_batch_size = _BATCH_SIZE_,\n",
    "    per_device_eval_batch_size  = 200,\n",
    "    output_dir                  = _OUTPUT_DIR_ + '/' + tasks[0],\n",
    "    save_total_limit            = 10,\n",
    "    overwrite_output_dir        = True,\n",
    "    remove_unused_columns       = False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = Test,\n",
    "    eval_dataset    = Test,\n",
    ")\n",
    "\n",
    "trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "ignore_keys = None\n",
    "if is_encoder_decoder:\n",
    "    ignore_keys = ['encoder_last_hidden_state']\n",
    "    \n",
    "results = trainer.predict(Test, ignore_keys = ignore_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404c186d",
   "metadata": {},
   "source": [
    "## Test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e4c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7491666666666666: 100%|██████████████| 2400/2400 [03:08<00:00, 12.73it/s]\n"
     ]
    }
   ],
   "source": [
    "from tools.Testing import compute_author_predictions_nli\n",
    "\n",
    "author_predictions = compute_author_predictions_nli(baseTest, results.predictions, 'gender', 2, nli_label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14eda0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Maria, Jose\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7001    0.8717    0.7765      1200\n",
      "           1     0.8300    0.6267    0.7142      1200\n",
      "\n",
      "    accuracy                         0.7492      2400\n",
      "   macro avg     0.7651    0.7492    0.7453      2400\n",
      "weighted avg     0.7651    0.7492    0.7453      2400\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6520    0.8742    0.7469      1200\n",
      "           1     0.8091    0.5333    0.6429      1200\n",
      "\n",
      "    accuracy                         0.7037      2400\n",
      "   macro avg     0.7305    0.7037    0.6949      2400\n",
      "weighted avg     0.7305    0.7037    0.6949      2400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = {'soft': classification_report(author_predictions['true'], author_predictions['pred_soft'], digits=4), \n",
    "                       'hard': classification_report(author_predictions['true'], author_predictions['pred_hard'], digits=4)}\n",
    "print(\"Results Maria, Jose\\n\\n\")\n",
    "print(report['soft'])\n",
    "print(report['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300023c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efede760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025a56d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f312395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13baf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc70e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
