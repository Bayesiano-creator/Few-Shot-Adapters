{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fb3dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The global seed 260615\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = 260615\n",
    "set_all_seeds(seed)\n",
    "\n",
    "print(\"The global seed \" + str(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a407e6",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0faaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGUAGE AND DATASET\n",
    "\n",
    "_LANGUAGE_         = 'en'\n",
    "_DATASET_          = '2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f81690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CLASSIFICATION\n",
    "\n",
    "_PRETRAINED_LM_    = 'vinai/bertweet-base'\n",
    "_PREPROCESS_TEXT_  = True\n",
    "_TWEET_BATCH_SIZE_ = 5\n",
    "_ADAPTER_CONFIG_   = transformers.ParallelConfig(reduction_factor = 256)\n",
    "_MAX_SEQ_LEN_      = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daabdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "_OUTPUT_DIR_       = 'checkPointsNLIES'\n",
    "_LOGGING_STEPS_    = 50\n",
    "_NUM_AUTHORS_      = 256\n",
    "_K_FOLD_CV_        = 5\n",
    "_NO_GPUS_          = 1\n",
    "_BATCH_SIZE_       = int(8 / _NO_GPUS_)\n",
    "_EPOCHS_           = 10\n",
    "_LEARNING_RATE_    = 1e-8\n",
    "\n",
    "# PREDICTIONS\n",
    "\n",
    "_DATASET_          = 'PAN17_NLI'\n",
    "_PRED_DIR_         = 'NLI_5tweet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c9214",
   "metadata": {},
   "source": [
    "## Other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b722fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DICTONARIES -----------------------------------------------------------------------\n",
    "\n",
    "# 2017\n",
    "\n",
    "gender_dict    = {'female': 0, 'male':   1}\n",
    "varietyEN_dict = {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}\n",
    "varietyES_dict = {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}  \n",
    "\n",
    "genderEN_hip  = {0: 'I’m a female', 1: 'I’m a male'}\n",
    "genderES_hip  = {0: 'Mi nombre es María', 1: 'Mi nombre es José'}\n",
    "\n",
    "# 2020 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc921f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LANGUAGE DICTIONARIES\n",
    "\n",
    "if _LANGUAGE_ == 'en':\n",
    "    gender_hip   = genderEN_hip\n",
    "    variety_dict = varietyEN_dict\n",
    "\n",
    "elif _LANGUAGE_ == 'es':\n",
    "    gender_hip   = genderES_hip\n",
    "    variety_dict = varietyES_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c35894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using a model of type roberta to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# SET LANGUAGE TOKENIZER\n",
    "\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(_PRETRAINED_LM_)\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "config             = PretrainedConfig.from_pretrained(_PRETRAINED_LM_)\n",
    "nli_label2id       = config.label2id\n",
    "is_encoder_decoder = config.is_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef9838",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1221c2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from random import shuffle, sample\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "from tools.TweetNormalizer import normalizeTweet\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BasePAN():\n",
    "    \n",
    "    def __init__(self, Dir, split, language, label_idx, label_dict):\n",
    "        self.Dir          = Dir\n",
    "        self.split        = split\n",
    "        self.language     = language\n",
    "        self.tokenizer    = tokenizer\n",
    "        self.label_dict   = label_dict\n",
    "        self.tw_bsz       = tweet_batch_size\n",
    "        \n",
    "        \n",
    "        self.authors   = self.get_authors(Dir, split, language)\n",
    "        self.author_lb = self.get_author_labels(Dir, split, language)\n",
    "        \n",
    "        self.author_ids = {}\n",
    "        for i in range(len(self.authors)):\n",
    "            self.author_ids[ self.authors[i] ] = i\n",
    "        \n",
    "        \n",
    "        # Save authors splited by gender ----------------------------------\n",
    "        \n",
    "        # create empty dictionary of authors per label\n",
    "        \n",
    "        self.splited_authors = {}\n",
    "        for i in gender_dict.values():\n",
    "            self.splited_authors[ i ] = []\n",
    "        \n",
    "        # fill dictionary \n",
    "        \n",
    "        for author in self.authors:\n",
    "            gl = self.author_lb[author]['gender']\n",
    "            self.splited_authors[ gl ].append(author)\n",
    "            \n",
    "        # shuffle authors\n",
    "        \n",
    "        for i in gender_dict.values():\n",
    "            shuffle(self.splited_authors[i])\n",
    "        \n",
    "        #----------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    def get_authors(self, Dir, split, language):\n",
    "        path    = os.path.join(Dir, split, language)\n",
    "        files   = os.listdir(path)\n",
    "        authors = [ file[0:-4] for file in files ] \n",
    "        \n",
    "        return authors\n",
    "    \n",
    "    \n",
    "    def get_author_labels(self, Dir, split, language):\n",
    "        lb_file_name = os.path.join(Dir, split, language + '.txt')\n",
    "        lb_file      = open(lb_file_name, \"r\")\n",
    "        author_lb    = dict()\n",
    "\n",
    "        for line in lb_file:\n",
    "            author, gender, variety = line.split(':::')\n",
    "            variety = variety[:-1]                       \n",
    "\n",
    "            gl = self.gender_dict[gender]\n",
    "            vl = self.variety_dict[variety]\n",
    "\n",
    "            author_lb[author] = {'gender': gl, 'variety': vl}\n",
    "\n",
    "        lb_file.close()\n",
    "        \n",
    "        return author_lb\n",
    "    \n",
    "    \n",
    "    def get_tweets_in_batches(self, Dir, split, language):\n",
    "        data   = []\n",
    "\n",
    "        for author in self.authors:\n",
    "            tw_file_name = os.path.join(Dir, split, language, author + '.xml')\n",
    "            tree         = ET.parse(tw_file_name)\n",
    "            root         = tree.getroot()\n",
    "            documents    = root[0]\n",
    "            total_tweets = len(documents)\n",
    "\n",
    "            for i in range(0, total_tweets, self.tw_bsz):\n",
    "                doc_batch = documents[i : i + self.tw_bsz]\n",
    "                tweets    = ''\n",
    "\n",
    "                for document in doc_batch:\n",
    "                    tweets += document.text + '\\n'\n",
    "\n",
    "                data.append( {'author': author, 'text': tweets, **self.author_lb[author]} )\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def get_tweets_in_batches_NLI(self, Dir, split, language):\n",
    "        data   = []\n",
    "\n",
    "        for author in self.authors:\n",
    "            tw_file_name = os.path.join(Dir, split, language, author + '.xml')\n",
    "            tree         = ET.parse(tw_file_name)\n",
    "            root         = tree.getroot()\n",
    "            documents    = root[0]\n",
    "            total_tweets = len(documents)\n",
    "\n",
    "            for i in range(0, total_tweets, self.tw_bsz):\n",
    "                doc_batch = documents[i : i + self.tw_bsz]\n",
    "                tweets    = ''\n",
    "\n",
    "                for document in doc_batch:\n",
    "                    tweets += document.text + '\\n'\n",
    "\n",
    "                data.append( {'author': author, 'text': tweets, **self.author_lb[author]} )\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def get_all_data():\n",
    "        \n",
    "        print(\"\\nReading data...\")\n",
    "        \n",
    "        self.data = self.get_tweets_in_batches(Dir, split, language)\n",
    "        \n",
    "        shuffle(self.data)\n",
    "        \n",
    "        if preprocess_text:\n",
    "            print(\"    Done\\nPreprocessing text...\")\n",
    "            \n",
    "            if self.language == 'es':\n",
    "                preprocessed   = [preprocess_tweet(instance['text']) for instance in self.data]\n",
    "            elif self.language == 'en':\n",
    "                preprocessed   = [normalizeTweet(instance['text'])   for instance in self.data]\n",
    "            \n",
    "        else:\n",
    "            preprocessed   = [instance['text'] for instance in self.data]\n",
    "        \n",
    "        print(\"    Done\\nTokenizing...\")\n",
    "        \n",
    "        self.encodings = self.tokenizer(preprocessed, max_length = max_seq_len, \n",
    "                                                      truncation = True, \n",
    "                                                      padding    = True,\n",
    "                                                      return_tensors = 'pt')\n",
    "         \n",
    "        print(\"    Done\\nMerging data...\")\n",
    "        \n",
    "        for i in range(len(self.data)):\n",
    "            self.data[i].update( {key: self.encodings[key][i] for key in self.encodings.keys()} )\n",
    "        \n",
    "        print(\"    Done\\n\\nTotal Instances: \" + str(len(self.data)) + '\\n')\n",
    "    \n",
    "    \n",
    "    def cross_val(self, k, val_idx, num_authors):\n",
    "        \n",
    "        if k > 1:\n",
    "            sz     = int(len(self.authors) / len(self.gender_dict))\n",
    "            val_sz = int(sz / k)\n",
    "        if k == 1:\n",
    "            sz     = int(len(self.authors) / len(self.gender_dict))\n",
    "            val_sz = 0\n",
    "        \n",
    "        splited_train = {}\n",
    "        splited_val   = {}\n",
    "        \n",
    "        for i in self.gender_dict.values():\n",
    "            splited_train[i] = self.splited_authors[i][0:( val_sz*val_idx )] + self.splited_authors[i][( val_sz*(val_idx+1) ):sz]\n",
    "            splited_val[i]   = self.splited_authors[i][( val_sz*val_idx ):( val_sz*(val_idx+1) )]\n",
    "        \n",
    "        authors_train = []\n",
    "        authors_val   = []\n",
    "        \n",
    "        for i in self.gender_dict.values():\n",
    "            authors_train += sample(splited_train[i], num_authors)\n",
    "            authors_val   += splited_val[i]\n",
    "        \n",
    "        \n",
    "        return authors_train, authors_val    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6438e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 419998\n",
      "\n",
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 280000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseTrain  = BasePAN17(Dir             = 'data/' + _DATASET_,\n",
    "                      split            = 'train',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad692bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
