{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5287a8d",
   "metadata": {},
   "source": [
    "## Set Global Seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e68a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The global seed 260615\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed = 260615\n",
    "set_all_seeds(seed)\n",
    "\n",
    "print(\"The global seed \" + str(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a407e6",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0faaf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LANGUAGE\n",
    "\n",
    "_LANGUAGE_         = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f81690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL CLASSIFICATION\n",
    "\n",
    "_PRETRAINED_LM_    = 'ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli'\n",
    "_PREPROCESS_TEXT_  = True\n",
    "_TWEET_BATCH_SIZE_ = 5\n",
    "_ADAPTER_CONFIG_   = transformers.ParallelConfig(reduction_factor = 256)\n",
    "_MAX_SEQ_LEN_      = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daabdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "\n",
    "_OUTPUT_DIR_       = 'checkPointsNLI'\n",
    "_LOGGING_STEPS_    = 50\n",
    "_NUM_AUTHORS_      = 8\n",
    "_K_FOLD_CV_        = 5\n",
    "_NO_GPUS_          = 1\n",
    "_BATCH_SIZE_       = int(8 / _NO_GPUS_)\n",
    "_EPOCHS_           = 10\n",
    "_LEARNING_RATE_    = 1e-8\n",
    "\n",
    "# PREDICTIONS\n",
    "\n",
    "_DATASET_          = 'PAN17_NLI'\n",
    "_PRED_DIR_         = 'NLI_5tweet'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2c9214",
   "metadata": {},
   "source": [
    "## Other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b722fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL DICTONARIES -----------------------------------------------------------------------\n",
    "\n",
    "gender_dict    = {'female': 0, 'male':   1}\n",
    "varietyEN_dict = {'australia': 0, 'canada': 1, 'great britain': 2, 'ireland': 3, 'new zealand': 4, 'united states': 5}\n",
    "varietyES_dict = {'argentina': 0, 'chile': 1, 'colombia': 2, 'mexico': 3, 'peru': 4, 'spain': 5, 'venezuela': 6}  \n",
    "\n",
    "genderEN_hip  = {0: 'I’m a female', 1: 'I’m a male'}\n",
    "genderES_hip  = {0: 'Mi nombre es María', 1: 'Mi nombre es José'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc921f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET LANGUAGE DICTIONARIES\n",
    "\n",
    "if _LANGUAGE_ == 'en':\n",
    "    gender_hip   = genderEN_hip\n",
    "    variety_dict = varietyEN_dict\n",
    "\n",
    "elif _LANGUAGE_ == 'es':\n",
    "    gender_hip   = genderES_hip\n",
    "    variety_dict = varietyES_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c35894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bart to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "# SET LANGUAGE TOKENIZER\n",
    "\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(_PRETRAINED_LM_)\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "config             = PretrainedConfig.from_pretrained(_PRETRAINED_LM_)\n",
    "nli_label2id       = config.label2id\n",
    "is_encoder_decoder = config.is_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef9838",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6438e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 360000\n",
      "\n",
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 240000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.DataLoaders import BasePAN17\n",
    "\n",
    "baseTrain  = BasePAN17(Dir             = 'data/2017',\n",
    "                      split            = 'train',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = 1,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_)\n",
    "\n",
    "baseTest  = BasePAN17(Dir              = 'data/2017',\n",
    "                      split            = 'test',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = 1,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a6a42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossVal_splits = []\n",
    "\n",
    "for val_idx in range(_K_FOLD_CV_):\n",
    "    \n",
    "    authors_train, authors_val = baseTrain.cross_val(_K_FOLD_CV_, val_idx, _NUM_AUTHORS_)\n",
    "    \n",
    "    crossVal_splits.append( (authors_train, authors_val) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02829600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 144000\n",
      "\n",
      "\n",
      "Reading data...\n",
      "    Done\n",
      "Preprocessing text...\n",
      "    Done\n",
      "Tokenizing...\n",
      "    Done\n",
      "Merging data...\n",
      "    Done\n",
      "\n",
      "Total Instances: 96000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tools.DataLoaders import BasePAN17nli\n",
    "\n",
    "baseTrain  = BasePAN17nli(Dir             = 'data/2017',\n",
    "                      split            = 'train',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_,\n",
    "                      label            = 'gender',\n",
    "                      label_hip        = gender_hip,\n",
    "                      nli_label2id     = nli_label2id)\n",
    "\n",
    "baseTest  = BasePAN17nli(Dir              = 'data/2017',\n",
    "                      split            = 'test',\n",
    "                      language         = _LANGUAGE_,\n",
    "                      tokenizer        = tokenizer,\n",
    "                      gender_dict      = gender_dict,\n",
    "                      variety_dict     = variety_dict,\n",
    "                      tweet_batch_size = _TWEET_BATCH_SIZE_,\n",
    "                      max_seq_len      = _MAX_SEQ_LEN_,\n",
    "                      preprocess_text  = _PREPROCESS_TEXT_,\n",
    "                      label            = 'gender',\n",
    "                      label_hip        = gender_hip,\n",
    "                      nli_label2id     = nli_label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae2a37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.DataLoaders import DatasetPAN17\n",
    "\n",
    "Test = DatasetPAN17(baseTest, 'gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ef19729",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3d8285a6183b250bf7810f1110ebd408',\n",
       " '75369e6c54e6b643c7b5112fe484d048',\n",
       " 'e15ff8259c2b18778594e47a4bce375a',\n",
       " 'b2e5086a0e2f263f48ba1bec23dcc32',\n",
       " '7c61c34e980e22bda49e63f235a08c50',\n",
       " '5a61761418a8db2ccdff2b2aacc3a64e',\n",
       " 'fa0d4331d8a79340d0720556f04dcc79',\n",
       " '8bbaf8237695dffe77a19e05d1bdc10c',\n",
       " '66eabf9f244ccf162fda0500d9d6891a',\n",
       " 'ecc242c3785dcfb89f71cafbc2607ead',\n",
       " 'ba6f1a42a9f0e593eb9a9ed239bae00',\n",
       " 'd82ec6fc92fccd520194d1ddd14ea2fe',\n",
       " 'c0fa91662b3c3a014136e483c5041dda',\n",
       " 'b51a60c4f5dd990bc9975c8fee9f3f1b',\n",
       " '9efac3abcc9f592c074fd8f214e2fcf4',\n",
       " '5a2567c48d7d3a4fde1beb6f8fcebe3c']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossVal_splits[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0a3700",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bfe625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "samples = 2 * _NUM_AUTHORS_ * int(100 / _TWEET_BATCH_SIZE_)\n",
    "_LOGGING_STEPS_ = int(samples / _BATCH_SIZE_)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate               = _LEARNING_RATE_,\n",
    "    num_train_epochs            = _EPOCHS_,\n",
    "    per_device_train_batch_size = _BATCH_SIZE_,\n",
    "    per_device_eval_batch_size  = 200,\n",
    "    logging_steps               = _LOGGING_STEPS_,\n",
    "    output_dir                  = _OUTPUT_DIR_,\n",
    "    save_total_limit            = 10,\n",
    "    overwrite_output_dir        = True,\n",
    "    remove_unused_columns       = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "716df656",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 13:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>8.036300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>8.115100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>7.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.496500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>7.749200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>7.671800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>7.092300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>6.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>7.167700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>6.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>7.020800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>6.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.766500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>6.701100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>6.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>6.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>6.947100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.260600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsNLI/checkpoint-500\n",
      "Configuration saved in checkPointsNLI/checkpoint-500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-21000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 28800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7458333333333333: 100%|██████████████████████████████████████████████| 720/720 [01:05<00:00, 10.97it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 640\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 1:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6855    0.9083    0.7814       360\n",
      "           1     0.8642    0.5833    0.6965       360\n",
      "\n",
      "    accuracy                         0.7458       720\n",
      "   macro avg     0.7749    0.7458    0.7389       720\n",
      "weighted avg     0.7749    0.7458    0.7389       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6680    0.8944    0.7648       360\n",
      "           1     0.8403    0.5556    0.6689       360\n",
      "\n",
      "    accuracy                         0.7250       720\n",
      "   macro avg     0.7542    0.7250    0.7169       720\n",
      "weighted avg     0.7542    0.7250    0.7169       720\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7888888888888889: 100%|██████████████████████████████████████████████| 720/720 [01:04<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 1 using LOGISTIC REGRESSION:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7430    0.8833    0.8071       360\n",
      "           1     0.8562    0.6944    0.7669       360\n",
      "\n",
      "    accuracy                         0.7889       720\n",
      "   macro avg     0.7996    0.7889    0.7870       720\n",
      "weighted avg     0.7996    0.7889    0.7870       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7822    0.7583    0.7701       360\n",
      "           1     0.7655    0.7889    0.7770       360\n",
      "\n",
      "    accuracy                         0.7736       720\n",
      "   macro avg     0.7739    0.7736    0.7736       720\n",
      "weighted avg     0.7739    0.7736    0.7736       720\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/bf704e14bcd921d2d4cfcad78a3add263a85a5d067122102d3add0fb620085c7.88e321f78373dda73f5c421340751fd102e1cf513f3e985ac0ca9a0865c4e94a\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/65b36d59291108e7b2b54b160e1b991a57571c5c0451bcc00892f7aa05f2f03b.7932c62c1631960fb4052ed59321fc8cb441e6e30612433c4fee8d99cc909473\n",
      "All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
      "\n",
      "All the weights of BartForSequenceClassification were initialized from the model checkpoint at ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForSequenceClassification for predictions without further training.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 13:08, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>8.618800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>8.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>8.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>8.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>7.982100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>7.980300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>7.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>7.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.479300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>7.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>7.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>7.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>7.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.996200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>7.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>7.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>7.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>7.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.942800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsNLI/checkpoint-500\n",
      "Configuration saved in checkPointsNLI/checkpoint-500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 28800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7277777777777777: 100%|██████████████████████████████████████████████| 720/720 [00:44<00:00, 16.34it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 640\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 2:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6660    0.9139    0.7705       360\n",
      "           1     0.8628    0.5417    0.6655       360\n",
      "\n",
      "    accuracy                         0.7278       720\n",
      "   macro avg     0.7644    0.7278    0.7180       720\n",
      "weighted avg     0.7644    0.7278    0.7180       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6515    0.9139    0.7607       360\n",
      "           1     0.8558    0.5111    0.6400       360\n",
      "\n",
      "    accuracy                         0.7125       720\n",
      "   macro avg     0.7536    0.7125    0.7003       720\n",
      "weighted avg     0.7536    0.7125    0.7003       720\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7625: 100%|██████████████████████████████████████████████████████████| 720/720 [01:25<00:00,  8.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 2 using LOGISTIC REGRESSION:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7926    0.7111    0.7496       360\n",
      "           1     0.7380    0.8139    0.7741       360\n",
      "\n",
      "    accuracy                         0.7625       720\n",
      "   macro avg     0.7653    0.7625    0.7619       720\n",
      "weighted avg     0.7653    0.7625    0.7619       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8408    0.5722    0.6810       360\n",
      "           1     0.6758    0.8917    0.7689       360\n",
      "\n",
      "    accuracy                         0.7319       720\n",
      "   macro avg     0.7583    0.7319    0.7249       720\n",
      "weighted avg     0.7583    0.7319    0.7249       720\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/bf704e14bcd921d2d4cfcad78a3add263a85a5d067122102d3add0fb620085c7.88e321f78373dda73f5c421340751fd102e1cf513f3e985ac0ca9a0865c4e94a\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/65b36d59291108e7b2b54b160e1b991a57571c5c0451bcc00892f7aa05f2f03b.7932c62c1631960fb4052ed59321fc8cb441e6e30612433c4fee8d99cc909473\n",
      "All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
      "\n",
      "All the weights of BartForSequenceClassification were initialized from the model checkpoint at ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForSequenceClassification for predictions without further training.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 12:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.542800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>8.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>8.491000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>8.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>8.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>8.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>7.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>7.673000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>7.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.567400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>7.577000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>7.411600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>7.147700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>7.523600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>7.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>7.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>7.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>7.192800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>7.195700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>7.079300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsNLI/checkpoint-500\n",
      "Configuration saved in checkPointsNLI/checkpoint-500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 28800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7180555555555556: 100%|██████████████████████████████████████████████| 720/720 [00:39<00:00, 18.42it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 640\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 3:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6632    0.8861    0.7586       360\n",
      "           1     0.8285    0.5500    0.6611       360\n",
      "\n",
      "    accuracy                         0.7181       720\n",
      "   macro avg     0.7458    0.7181    0.7099       720\n",
      "weighted avg     0.7458    0.7181    0.7099       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6447    0.8972    0.7503       360\n",
      "           1     0.8311    0.5056    0.6287       360\n",
      "\n",
      "    accuracy                         0.7014       720\n",
      "   macro avg     0.7379    0.7014    0.6895       720\n",
      "weighted avg     0.7379    0.7014    0.6895       720\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7333333333333333: 100%|██████████████████████████████████████████████| 720/720 [01:20<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 3 using LOGISTIC REGRESSION:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7485    0.7028    0.7249       360\n",
      "           1     0.7199    0.7639    0.7412       360\n",
      "\n",
      "    accuracy                         0.7333       720\n",
      "   macro avg     0.7342    0.7333    0.7331       720\n",
      "weighted avg     0.7342    0.7333    0.7331       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7270    0.6583    0.6910       360\n",
      "           1     0.6878    0.7528    0.7188       360\n",
      "\n",
      "    accuracy                         0.7056       720\n",
      "   macro avg     0.7074    0.7056    0.7049       720\n",
      "weighted avg     0.7074    0.7056    0.7049       720\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/bf704e14bcd921d2d4cfcad78a3add263a85a5d067122102d3add0fb620085c7.88e321f78373dda73f5c421340751fd102e1cf513f3e985ac0ca9a0865c4e94a\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/65b36d59291108e7b2b54b160e1b991a57571c5c0451bcc00892f7aa05f2f03b.7932c62c1631960fb4052ed59321fc8cb441e6e30612433c4fee8d99cc909473\n",
      "All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
      "\n",
      "All the weights of BartForSequenceClassification were initialized from the model checkpoint at ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForSequenceClassification for predictions without further training.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 13:47, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>7.798500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>7.561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>7.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>7.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>7.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>6.853600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>6.637300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>6.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>6.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>6.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.625100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>6.392400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>6.482600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>6.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>6.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.765900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsNLI/checkpoint-500\n",
      "Configuration saved in checkPointsNLI/checkpoint-500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 28800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7277777777777777: 100%|██████████████████████████████████████████████| 720/720 [00:54<00:00, 13.11it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 640\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 4:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6694    0.9000    0.7678       360\n",
      "           1     0.8475    0.5556    0.6711       360\n",
      "\n",
      "    accuracy                         0.7278       720\n",
      "   macro avg     0.7584    0.7278    0.7195       720\n",
      "weighted avg     0.7584    0.7278    0.7195       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6519    0.9000    0.7561       360\n",
      "           1     0.8386    0.5194    0.6415       360\n",
      "\n",
      "    accuracy                         0.7097       720\n",
      "   macro avg     0.7452    0.7097    0.6988       720\n",
      "weighted avg     0.7452    0.7097    0.6988       720\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7736111111111111: 100%|██████████████████████████████████████████████| 720/720 [00:34<00:00, 21.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 4 using LOGISTIC REGRESSION:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7872    0.7500    0.7681       360\n",
      "           1     0.7613    0.7972    0.7788       360\n",
      "\n",
      "    accuracy                         0.7736       720\n",
      "   macro avg     0.7742    0.7736    0.7735       720\n",
      "weighted avg     0.7742    0.7736    0.7735       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8195    0.6306    0.7127       360\n",
      "           1     0.6998    0.8611    0.7721       360\n",
      "\n",
      "    accuracy                         0.7458       720\n",
      "   macro avg     0.7596    0.7458    0.7424       720\n",
      "weighted avg     0.7596    0.7458    0.7424       720\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/bf704e14bcd921d2d4cfcad78a3add263a85a5d067122102d3add0fb620085c7.88e321f78373dda73f5c421340751fd102e1cf513f3e985ac0ca9a0865c4e94a\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/65b36d59291108e7b2b54b160e1b991a57571c5c0451bcc00892f7aa05f2f03b.7932c62c1631960fb4052ed59321fc8cb441e6e30612433c4fee8d99cc909473\n",
      "All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
      "\n",
      "All the weights of BartForSequenceClassification were initialized from the model checkpoint at ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForSequenceClassification for predictions without further training.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 640\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 800\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 14:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>8.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>7.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>8.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.664100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>7.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>7.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>7.528200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>7.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>7.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>7.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>7.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>7.061400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>6.852900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.918000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>6.758500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>6.634500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>6.912600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>6.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.918100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsNLI/checkpoint-500\n",
      "Configuration saved in checkPointsNLI/checkpoint-500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 28800\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7458333333333333: 100%|██████████████████████████████████████████████| 720/720 [01:13<00:00,  9.81it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 640\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 5:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6911    0.8889    0.7776       360\n",
      "           1     0.8444    0.6028    0.7034       360\n",
      "\n",
      "    accuracy                         0.7458       720\n",
      "   macro avg     0.7678    0.7458    0.7405       720\n",
      "weighted avg     0.7678    0.7458    0.7405       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6758    0.8917    0.7689       360\n",
      "           1     0.8408    0.5722    0.6810       360\n",
      "\n",
      "    accuracy                         0.7319       720\n",
      "   macro avg     0.7583    0.7319    0.7249       720\n",
      "weighted avg     0.7583    0.7319    0.7249       720\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7666666666666667: 100%|██████████████████████████████████████████████| 720/720 [00:42<00:00, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 5 using LOGISTIC REGRESSION:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7945    0.7194    0.7551       360\n",
      "           1     0.7437    0.8139    0.7772       360\n",
      "\n",
      "    accuracy                         0.7667       720\n",
      "   macro avg     0.7691    0.7667    0.7661       720\n",
      "weighted avg     0.7691    0.7667    0.7661       720\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8092    0.6361    0.7123       360\n",
      "           1     0.7002    0.8500    0.7679       360\n",
      "\n",
      "    accuracy                         0.7431       720\n",
      "   macro avg     0.7547    0.7431    0.7401       720\n",
      "weighted avg     0.7547    0.7431    0.7401       720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from tools.DataLoaders import DatasetCrossValnli\n",
    "from transformers import Trainer\n",
    "from tools.Testing import compute_author_predictions_nli, compute_author_predictions_nli_LR\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "task = 'gender'\n",
    "\n",
    "f1s_soft = []\n",
    "f1s_hard = []\n",
    "\n",
    "f1s_soft_LR = []\n",
    "f1s_hard_LR = []\n",
    "\n",
    "for split in range( _K_FOLD_CV_ ):\n",
    "    \n",
    "    # loaders for current split ------------------------------------------\n",
    "    \n",
    "    authors_train, authors_val = crossVal_splits[split]\n",
    "    \n",
    "    Train = DatasetCrossValnli(baseTrain, authors_train)\n",
    "    Val   = DatasetCrossValnli(baseTrain, authors_val)\n",
    "    \n",
    "    \n",
    "    # initialize model ---------------------------------------------------\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(_PRETRAINED_LM_)\n",
    "    \n",
    "    \n",
    "    # create trainer and train -------------------------------------------\n",
    "        \n",
    "    trainer = Trainer(\n",
    "        model           = model,\n",
    "        args            = training_args,\n",
    "        train_dataset   = Train,\n",
    "    )\n",
    "    trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "    # get predictions ----------------------------------------------------\n",
    "    \n",
    "    \n",
    "    ignore_keys = None\n",
    "    if is_encoder_decoder:\n",
    "        ignore_keys = ['encoder_last_hidden_state']\n",
    "\n",
    "    results            = trainer.predict(Val , ignore_keys = ignore_keys)\n",
    "    author_predictions = compute_author_predictions_nli(Val, results.predictions, 'gender', 2, nli_label2id)\n",
    "\n",
    "    # report metrics \n",
    "\n",
    "    report = {'soft': classification_report(author_predictions['true'], author_predictions['pred_soft'], digits=4), \n",
    "               'hard': classification_report(author_predictions['true'], author_predictions['pred_hard'], digits=4)}\n",
    "\n",
    "    f1s_soft.append( f1_score(author_predictions['true'], author_predictions['pred_soft'], average = 'macro') )\n",
    "    f1s_hard.append( f1_score(author_predictions['true'], author_predictions['pred_hard'], average = 'macro') )\n",
    "    \n",
    "    print(\"Results with split \" + str(split + 1) + \":\\n\")\n",
    "    print(\"soft voting:\\n\", report['soft'], '\\n')\n",
    "    print(\"hard voting:\\n\", report['hard'])\n",
    "\n",
    "\n",
    "    # get predictions with Logistic Regression----------------------------\n",
    "\n",
    "    resultsTrain = trainer.predict(Train, ignore_keys = ignore_keys)\n",
    "    author_predictions_LR = compute_author_predictions_nli_LR(Train, Val, resultsTrain.predictions, results.predictions, 'gender', 2)\n",
    "    \n",
    "    f1s_soft_LR.append( f1_score(author_predictions_LR['true'], author_predictions_LR['pred_soft'], average = 'macro') )\n",
    "    f1s_hard_LR.append( f1_score(author_predictions_LR['true'], author_predictions_LR['pred_hard'], average = 'macro') )\n",
    "    \n",
    "    # report metrics \n",
    "\n",
    "    report_LR = {'soft': classification_report(author_predictions_LR['true'], author_predictions_LR['pred_soft'], digits=4), \n",
    "               'hard': classification_report(author_predictions_LR['true'], author_predictions_LR['pred_hard'], digits=4)}\n",
    "\n",
    "    print(\"Results with split \" + str(split + 1) + \" using LOGISTIC REGRESSION:\\n\")\n",
    "    print(\"soft voting:\\n\", report_LR['soft'], '\\n')\n",
    "    print(\"hard voting:\\n\", report_LR['hard'])\n",
    "\n",
    "     \n",
    "    \n",
    "    # save predictions ----------------------------------------------------\n",
    "    \n",
    "    DIR = 'results/' + _DATASET_ + '/' + _LANGUAGE_ + '/' + _PRED_DIR_ + '/' + str(_NUM_AUTHORS_) + '_authors/split_' + str(split + 1) + '/'\n",
    "    if not os.path.exists(DIR):\n",
    "        os.makedirs(DIR)\n",
    "\n",
    "    with open(DIR + 'predictions.pickle', 'wb') as f:\n",
    "        pickle.dump(author_predictions, f)\n",
    "    \n",
    "    with open(DIR + 'predictions_LR.pickle', 'wb') as f:\n",
    "        pickle.dump(author_predictions_LR, f)\n",
    "\n",
    "    with open(DIR + 'report.txt', 'w') as f:\n",
    "        f.write(\"soft voting:\\n\" + report['soft'] + '\\n\\n')\n",
    "        f.write(\"hard voting:\\n\" + report['hard'])\n",
    "        \n",
    "    with open(DIR + 'report_LR.txt', 'w') as f:\n",
    "        f.write(\"soft voting:\\n\" + report_LR['soft'] + '\\n\\n')\n",
    "        f.write(\"hard voting:\\n\" + report_LR['hard'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a56aa6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft results:  [0.7389397100518911, 0.7180104067587981, 0.7098612630245812, 0.7194567257228283, 0.7405231679962818]\n",
      "\n",
      "Hard results:  [0.7168709633854733, 0.7003468208092485, 0.6894802404722788, 0.6988177274828824, 0.7249270054931459]\n",
      "\n",
      "\n",
      "Soft statistics: \n",
      "\t[avg, std]: [0.7253582547108761, 0.012193491619569664]\n",
      "\n",
      "Hard statistics: \n",
      "\t[avg, std]: [0.7060885515286058, 0.012905937419082796]\n"
     ]
    }
   ],
   "source": [
    "# report statistics\n",
    "\n",
    "print('Soft results: ', f1s_soft)\n",
    "print('\\nHard results: ', f1s_hard)\n",
    "\n",
    "f1s_soft = np.array(f1s_soft)\n",
    "f1s_hard = np.array(f1s_hard)\n",
    "\n",
    "FewShot_Results = {'soft': [f1s_soft.mean(), f1s_soft.std()], 'hard': [f1s_hard.mean(), f1s_hard.std()]}\n",
    "\n",
    "print('\\n\\nSoft statistics: ')\n",
    "print('\\t[avg, std]:', FewShot_Results['soft'])\n",
    "\n",
    "print('\\nHard statistics: ')\n",
    "print('\\t[avg, std]:', FewShot_Results['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49dda12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft results:  [0.7869888823144717, 0.7618711450570663, 0.733084129472733, 0.773484832400779, 0.7661451848643968]\n",
      "\n",
      "Hard results:  [0.7735582572321087, 0.7249270054931459, 0.7048974951860244, 0.7424102316470222, 0.7400828534687818]\n",
      "\n",
      "\n",
      "Soft statistics with LOGISTIC REGRESSION: \n",
      "\t[avg, std]: [0.7643148348218893, 0.01779258296506357]\n",
      "\n",
      "Hard statistics with LOGISTIC REGRESSION: \n",
      "\t[avg, std]: [0.7371751686054167, 0.022589623083733315]\n"
     ]
    }
   ],
   "source": [
    "# report statistics\n",
    "\n",
    "print('Soft results: ', f1s_soft_LR)\n",
    "print('\\nHard results: ', f1s_hard_LR)\n",
    "\n",
    "f1s_soft_LR = np.array(f1s_soft_LR)\n",
    "f1s_hard_LR = np.array(f1s_hard_LR)\n",
    "\n",
    "FewShot_Results_LR = {'soft': [f1s_soft_LR.mean(), f1s_soft_LR.std()], 'hard': [f1s_hard_LR.mean(), f1s_hard_LR.std()]}\n",
    "\n",
    "print('\\n\\nSoft statistics with LOGISTIC REGRESSION: ')\n",
    "print('\\t[avg, std]:', FewShot_Results_LR['soft'])\n",
    "\n",
    "print('\\nHard statistics with LOGISTIC REGRESSION: ')\n",
    "print('\\t[avg, std]:', FewShot_Results_LR['hard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d0e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aade6b3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfa5522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/config.json from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/bf704e14bcd921d2d4cfcad78a3add263a85a5d067122102d3add0fb620085c7.88e321f78373dda73f5c421340751fd102e1cf513f3e985ac0ca9a0865c4e94a\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"entailment\",\n",
      "    \"1\": \"neutral\",\n",
      "    \"2\": \"contradiction\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"contradiction\": 2,\n",
      "    \"entailment\": 0,\n",
      "    \"neutral\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": false,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"prefix\": \" \",\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli/resolve/main/pytorch_model.bin from cache at /001/usuarios/isaac.bribiesca/.cache/huggingface/transformers/65b36d59291108e7b2b54b160e1b991a57571c5c0451bcc00892f7aa05f2f03b.7932c62c1631960fb4052ed59321fc8cb441e6e30612433c4fee8d99cc909473\n",
      "All model checkpoint weights were used when initializing BartForSequenceClassification.\n",
      "\n",
      "All the weights of BartForSequenceClassification were initialized from the model checkpoint at ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForSequenceClassification for predictions without further training.\n",
      "/001/usuarios/isaac.bribiesca/anaconda3/envs/NLP/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20480\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25600' max='25600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25600/25600 3:45:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>5.547000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>1.843900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>1.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>0.931700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.869700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7680</td>\n",
       "      <td>0.806100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8960</td>\n",
       "      <td>0.780200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10240</td>\n",
       "      <td>0.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11520</td>\n",
       "      <td>0.744700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>0.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14080</td>\n",
       "      <td>0.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15360</td>\n",
       "      <td>0.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16640</td>\n",
       "      <td>0.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17920</td>\n",
       "      <td>0.714500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>0.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20480</td>\n",
       "      <td>0.715800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21760</td>\n",
       "      <td>0.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23040</td>\n",
       "      <td>0.715100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24320</td>\n",
       "      <td>0.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>0.710100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsNLI/checkpoint-500\n",
      "Configuration saved in checkPointsNLI/checkpoint-500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-1000\n",
      "Configuration saved in checkPointsNLI/checkpoint-1000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-1500\n",
      "Configuration saved in checkPointsNLI/checkpoint-1500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-2000\n",
      "Configuration saved in checkPointsNLI/checkpoint-2000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-2500\n",
      "Configuration saved in checkPointsNLI/checkpoint-2500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-3000\n",
      "Configuration saved in checkPointsNLI/checkpoint-3000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-3500\n",
      "Configuration saved in checkPointsNLI/checkpoint-3500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-4000\n",
      "Configuration saved in checkPointsNLI/checkpoint-4000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-4500\n",
      "Configuration saved in checkPointsNLI/checkpoint-4500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-5000\n",
      "Configuration saved in checkPointsNLI/checkpoint-5000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-5500\n",
      "Configuration saved in checkPointsNLI/checkpoint-5500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-6000\n",
      "Configuration saved in checkPointsNLI/checkpoint-6000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-6500\n",
      "Configuration saved in checkPointsNLI/checkpoint-6500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-7000\n",
      "Configuration saved in checkPointsNLI/checkpoint-7000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-7500\n",
      "Configuration saved in checkPointsNLI/checkpoint-7500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-8000\n",
      "Configuration saved in checkPointsNLI/checkpoint-8000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-8500\n",
      "Configuration saved in checkPointsNLI/checkpoint-8500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-9000\n",
      "Configuration saved in checkPointsNLI/checkpoint-9000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-9500\n",
      "Configuration saved in checkPointsNLI/checkpoint-9500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-10000\n",
      "Configuration saved in checkPointsNLI/checkpoint-10000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-10500\n",
      "Configuration saved in checkPointsNLI/checkpoint-10500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-11000\n",
      "Configuration saved in checkPointsNLI/checkpoint-11000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-11500\n",
      "Configuration saved in checkPointsNLI/checkpoint-11500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-12000\n",
      "Configuration saved in checkPointsNLI/checkpoint-12000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-12500\n",
      "Configuration saved in checkPointsNLI/checkpoint-12500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-13000\n",
      "Configuration saved in checkPointsNLI/checkpoint-13000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-13500\n",
      "Configuration saved in checkPointsNLI/checkpoint-13500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-14000\n",
      "Configuration saved in checkPointsNLI/checkpoint-14000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-14500\n",
      "Configuration saved in checkPointsNLI/checkpoint-14500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-9500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to checkPointsNLI/checkpoint-15000\n",
      "Configuration saved in checkPointsNLI/checkpoint-15000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-15500\n",
      "Configuration saved in checkPointsNLI/checkpoint-15500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-16000\n",
      "Configuration saved in checkPointsNLI/checkpoint-16000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-16500\n",
      "Configuration saved in checkPointsNLI/checkpoint-16500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-17000\n",
      "Configuration saved in checkPointsNLI/checkpoint-17000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-17500\n",
      "Configuration saved in checkPointsNLI/checkpoint-17500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-18000\n",
      "Configuration saved in checkPointsNLI/checkpoint-18000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-18500\n",
      "Configuration saved in checkPointsNLI/checkpoint-18500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-19000\n",
      "Configuration saved in checkPointsNLI/checkpoint-19000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-19500\n",
      "Configuration saved in checkPointsNLI/checkpoint-19500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-20000\n",
      "Configuration saved in checkPointsNLI/checkpoint-20000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-20500\n",
      "Configuration saved in checkPointsNLI/checkpoint-20500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-21000\n",
      "Configuration saved in checkPointsNLI/checkpoint-21000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-21500\n",
      "Configuration saved in checkPointsNLI/checkpoint-21500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-22000\n",
      "Configuration saved in checkPointsNLI/checkpoint-22000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-22500\n",
      "Configuration saved in checkPointsNLI/checkpoint-22500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-23000\n",
      "Configuration saved in checkPointsNLI/checkpoint-23000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-23500\n",
      "Configuration saved in checkPointsNLI/checkpoint-23500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-24000\n",
      "Configuration saved in checkPointsNLI/checkpoint-24000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-24500\n",
      "Configuration saved in checkPointsNLI/checkpoint-24500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-25000\n",
      "Configuration saved in checkPointsNLI/checkpoint-25000/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to checkPointsNLI/checkpoint-25500\n",
      "Configuration saved in checkPointsNLI/checkpoint-25500/config.json\n",
      "Model weights saved in checkPointsNLI/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [checkPointsNLI/checkpoint-20500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 96000\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7895833333333333: 100%|████████████████████████████████████████████| 2400/2400 [05:10<00:00,  7.73it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 20480\n",
      "  Batch size = 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 1:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7647    0.8367    0.7990      1200\n",
      "           1     0.8197    0.7425    0.7792      1200\n",
      "\n",
      "    accuracy                         0.7896      2400\n",
      "   macro avg     0.7922    0.7896    0.7891      2400\n",
      "weighted avg     0.7922    0.7896    0.7891      2400\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7598    0.8067    0.7825      1200\n",
      "           1     0.7940    0.7450    0.7687      1200\n",
      "\n",
      "    accuracy                         0.7758      2400\n",
      "   macro avg     0.7769    0.7758    0.7756      2400\n",
      "weighted avg     0.7769    0.7758    0.7756      2400\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "acc: 0.7895833333333333: 100%|████████████████████████████████████████████| 2400/2400 [05:18<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with split 1 using LOGISTIC REGRESSION:\n",
      "\n",
      "soft voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7870    0.7942    0.7905      1200\n",
      "           1     0.7923    0.7850    0.7886      1200\n",
      "\n",
      "    accuracy                         0.7896      2400\n",
      "   macro avg     0.7896    0.7896    0.7896      2400\n",
      "weighted avg     0.7896    0.7896    0.7896      2400\n",
      " \n",
      "\n",
      "hard voting:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7867    0.7683    0.7774      1200\n",
      "           1     0.7736    0.7917    0.7825      1200\n",
      "\n",
      "    accuracy                         0.7800      2400\n",
      "   macro avg     0.7802    0.7800    0.7800      2400\n",
      "weighted avg     0.7802    0.7800    0.7800      2400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from tools.DataLoaders import DatasetCrossValnli\n",
    "from transformers import Trainer\n",
    "from tools.Testing import compute_author_predictions_nli, compute_author_predictions_nli_LR\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "task = 'gender'\n",
    "\n",
    "split = 0\n",
    "    \n",
    "# loaders for current split ------------------------------------------\n",
    "\n",
    "authors_train, authors_val = crossVal_splits[split]\n",
    "\n",
    "Train = DatasetCrossValnli(baseTrain, authors_train)\n",
    "Val   = DatasetCrossValnli(baseTrain, authors_val)\n",
    "\n",
    "\n",
    "# initialize model ---------------------------------------------------\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(_PRETRAINED_LM_)\n",
    "\n",
    "\n",
    "# create trainer and train -------------------------------------------\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = Train,\n",
    ")\n",
    "trainer.args._n_gpu = _NO_GPUS_\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# get predictions ----------------------------------------------------\n",
    "\n",
    "ignore_keys = None\n",
    "if is_encoder_decoder:\n",
    "    ignore_keys = ['encoder_last_hidden_state']\n",
    "\n",
    "results            = trainer.predict(Test , ignore_keys = ignore_keys)\n",
    "author_predictions = compute_author_predictions_nli(baseTest, results.predictions, 'gender', 2, nli_label2id)\n",
    "\n",
    "# report metrics \n",
    "\n",
    "report = {'soft': classification_report(author_predictions['true'], author_predictions['pred_soft'], digits=4), \n",
    "           'hard': classification_report(author_predictions['true'], author_predictions['pred_hard'], digits=4)}\n",
    "\n",
    "print(\"Results with split \" + str(split + 1) + \":\\n\")\n",
    "print(\"soft voting:\\n\", report['soft'], '\\n')\n",
    "print(\"hard voting:\\n\", report['hard'])\n",
    "\n",
    "\n",
    "# get predictions with Logistic Regression----------------------------\n",
    "\n",
    "resultsTrain = trainer.predict(Train, ignore_keys = ignore_keys)\n",
    "author_predictions_LR = compute_author_predictions_nli_LR(Train, baseTest, resultsTrain.predictions, results.predictions, 'gender', 2)\n",
    "\n",
    "# report metrics \n",
    "\n",
    "report_LR = {'soft': classification_report(author_predictions_LR['true'], author_predictions_LR['pred_soft'], digits=4), \n",
    "           'hard': classification_report(author_predictions_LR['true'], author_predictions_LR['pred_hard'], digits=4)}\n",
    "\n",
    "print(\"Results with split \" + str(split + 1) + \" using LOGISTIC REGRESSION:\\n\")\n",
    "print(\"soft voting:\\n\", report_LR['soft'], '\\n')\n",
    "print(\"hard voting:\\n\", report_LR['hard'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa14dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
